{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ks4BKH9BRe4_",
    "colab_type": "text"
   },
   "source": [
    "╔══<i><b>&nbsp;Alai-DeepLearning&nbsp;</b></i>══════════════════════════════════╗\n",
    "###  &nbsp;&nbsp; **✎&nbsp;&nbsp;Week 9_cnn_basis**\n",
    "# Homework 1. emnist 을 cnn 으로 학습시키기\n",
    "\n",
    "\n",
    "### _Objective_\n",
    "1. homework 목적\n",
    "이전에 학습 했던 것을 기반으로 좀 더 모델을 발전시켜 봅니다.\n",
    "모델을 학습시켜 95% 이상 정확도를 가진 분류기를 만들어 봅니다. \n",
    "\n",
    "\n",
    "╚═══════════════════════════════════════════════╝\n",
    "\n",
    "---------\n",
    "\n",
    "# 실행 순서 \n",
    "------\n",
    "+ ### 1. DataDownload \n",
    "+ ### 2. EDA \n",
    " #### 1. train, test pixel 값의 EDA\n",
    " #### 2. train ,test class 별 데이터 갯수 \n",
    "\n",
    "+ ### 3. CNN 구성\n",
    " - **layer 별 학습**\n",
    " - **output ch 갯수 별 학습**\n",
    "    - [16, 32, 64, 128 ,26]\n",
    " - **layer 순서별 kernel size** \n",
    "    - (5,5) (3,3), (3,3), (3,3), (3,3)\n",
    " - **Regularization 적용**\n",
    "    - L2, Regularization : 0.0005\n",
    " - **batch 별 학습**\n",
    "    - 120 \n",
    " - **Optimizer**\n",
    "    - Gradient Descent optimizer \n",
    "\n",
    "+ ### 4. metric 측정 \n",
    " - (tensorboard accuracy, loss step별로 측정)\n",
    "\n",
    "+ ### 5. analysis \n",
    " - 각 층별 weights, bias 평균의 변화 측정 (step 별)*이탤릭체 텍스트*\n",
    "------\n",
    "\n",
    "\n",
    "# 실행방법: \n",
    "\n",
    "- DNN 파트를 CNN으로 변경한 후 다시 최적의 파라미터를 찾습니다.<br>\n",
    "그 후 가장 최적의 hyper parameter 을 찾은 후 optimizer 을 바꿔가며 최적화 해 보세요!\n",
    "\n",
    "+ ###  HINT 1\n",
    "\n",
    "\n",
    "\n",
    "# 제출 포맷 및 예\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZxGJKYeeQ9_",
    "colab_type": "text"
   },
   "source": [
    "### 1. EMNIST DataDownLOAD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "dMrmssZfQzxh",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "!pip install emnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "tb_xpB9MQ5-q",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import keras \n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "kQ-hEcJURVJK",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# show sample data \n",
    "print(train_images.shape)\n",
    "plt.imshow(train_images[0])\n",
    "plt.show()\n",
    "plt.imshow(train_images[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TgDylTThGF4",
    "colab_type": "text"
   },
   "source": [
    "#### Image EDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "F4rXGPknhSwr",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# Train Image \n",
    "n_train = len(train_images)\n",
    "seq_length = train_images.shape[1:3]\n",
    "\n",
    "train_min = train_images.reshape([-1,784]).min(axis=1)\n",
    "train_max = train_images.reshape([-1,784]).max(axis=1)\n",
    "\n",
    "print('# train : {}'.format(n_train))\n",
    "print('seq_length : {}'.format(seq_length))\n",
    "\n",
    "# 아래 그림으로 보아 모든 Test 이미지가 최소값은 0, 최대값은 255을 가지고 있음. \n",
    "plt.scatter(train_min, train_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "YxJIXlDGkWkG",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# Test Image \n",
    "n_test = len(test_images)\n",
    "seq_length = test_images.shape[1:3]\n",
    "\n",
    "test_min = test_images.reshape([-1,784]).min(axis=1)\n",
    "test_max = test_images.reshape([-1,784]).max(axis=1)\n",
    "\n",
    "print('# train : {}'.format(n_test))\n",
    "print('seq_length : {}'.format(seq_length))\n",
    "\n",
    "# 아래 그림으로 보아 모든 Test 이미지가 최소값은 0, 최대값은 255을 가지고 있음. \n",
    "plt.scatter(test_min, test_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hox-JI_akSfw",
    "colab_type": "text"
   },
   "source": [
    "#### Label EDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "K47CeLpSjBsB",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "print('train label data shape : {}'.format(train_labels.shape))\n",
    "print('test label data shape : {}'.format(test_labels.shape))\n",
    "\n",
    "print(np.unique(train_labels, return_counts=True))\n",
    "print(np.unique(test_labels, return_counts=True))\n",
    "\n",
    "num_outputs = len(np.unique(test_labels, return_counts=True)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fqpZfH91hTWf",
    "colab_type": "text"
   },
   "source": [
    "### Normalize and Normalization checking  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "-qUP6B8KS3t0",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "np.unique([train_labels],return_counts=True)\n",
    "\n",
    "# Normalization\n",
    "train_images = train_images.reshape(-1,784) / 255.\n",
    "test_images = test_images.reshape(-1,784) / 255.\n",
    "\n",
    "# Check Normalization \n",
    "def chk_norm(input_data, min_value=0, max_value=1):\n",
    "    \"\"\"\n",
    "    input_data : Ndarray, \n",
    "    min_value : int, input_data의 모든 element는 min_value 이상의 값을 가져야함 \n",
    "    max_value : int, input_data의 모든 element는 max_value 이하의 값을 가져야함 \n",
    "\n",
    "    description: input_data의 모든 element가 특정 범위(min_value ~ max_value)에 있는지 확인합니다.\n",
    "    \"\"\"\n",
    "    input_data = np.asarray(input_data)\n",
    "    assert np.all(input_data <= max_value) & np.all(input_data >= min_value), 'normalize가 잘못 되었습니다.'\n",
    "\n",
    "\n",
    "chk_norm(train_images)    \n",
    "chk_norm(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PoBOUBz6EDEz",
    "colab_type": "text"
   },
   "source": [
    "## Data reshape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "rf4NF5FPEFm3",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "train_images=train_images.reshape([-1, 28, 28, 1])\n",
    "test_images=test_images.reshape([-1, 28, 28, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EILqz9ejk3tf",
    "colab_type": "text"
   },
   "source": [
    "## Input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "h66szIRgcCa9",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "num_inputs = 28*28 # MNIST Input size\n",
    "num_outputs = 26 # The number of Label : 26\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Data를 받아오는 placeholder\n",
    "x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1],name='x')\n",
    "\n",
    "#labels shape : [1,3,7,7,4,..]\n",
    "labels_cls = tf.placeholder(tf.int32, shape=[None], name='labels') \n",
    "\n",
    "# scalar 을 onehot-vector 형태로 변환합니다.\n",
    "labels = tf.one_hot(labels_cls, depth=num_outputs)\n",
    "\n",
    "# Dropout 시 units이 0으로 될 확률.\n",
    "drop_rate = tf.placeholder(tf.float32, shape=[], name='drop_rate')\n",
    "\n",
    "# Learning Rate\n",
    "learning_rate = tf.placeholder_with_default(0.1,shape=(), name='learning_rate')\n",
    "\n",
    "# TODO: shape 을 고정하지 않고 하는 방법도 생각해 보세요\n",
    "num_layers=5\n",
    "l2_lambda = tf.placeholder_with_default(0.0, shape=[], name='l2_lambda') \n",
    "l1_lambda = tf.placeholder_with_default(0.0, shape=[], name='l1_lambda')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDJ5cNp7pSrU",
    "colab_type": "text"
   },
   "source": [
    "# BUILD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "aAmNm5S0Epwr",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# CNN Archithecture\n",
    "# layer 순서별 kernel_size \n",
    "kernel_sizes = [(5,5),(3,3),(3,3),(3,3)]\n",
    "conv_layer_units = [16, 32, 64, 128]\n",
    "\n",
    "# layer 길이 \n",
    "n_layers = len(conv_layer_units)\n",
    "\n",
    "\n",
    "# conv layer 구축\n",
    "in_ch = 1\n",
    "layer = x \n",
    "for layer_idx in range(n_layers):\n",
    "    with tf.variable_scope('conv_{}'.format(layer_idx)):\n",
    "\n",
    "        # output channels\n",
    "        out_ch = conv_layer_units[layer_idx]\n",
    "        \n",
    "        # kernel \n",
    "        k_h, k_w = kernel_sizes[layer_idx]\n",
    "        kernel = tf.Variable(tf.random.normal([k_h, k_w, in_ch, out_ch],\n",
    "                                              stddev=np.sqrt(2/num_inputs)),\n",
    "                             name='weight')\n",
    "\n",
    "        # 추후 weights 만 꺼내 사용하기 위해 weights 을 따로 보관함.          \n",
    "        tf.add_to_collection('weights', kernel)\n",
    "        \n",
    "        # bias \n",
    "        b = tf.Variable(tf.zeros([out_ch]), name='bias')\n",
    "\n",
    "        # Convolution\n",
    "        layer = tf.nn.conv2d(layer, kernel, strides=[1,1,1,1], padding='SAME') + b\n",
    "        \n",
    "        # Activation\n",
    "        layer = tf.nn.relu(layer)\n",
    "        \n",
    "        # max pooling : 이미지를 1/2 로 줄입니다. \n",
    "        layer = tf.nn.max_pool(layer, ksize=[1,2,2,1], strides=[1,2,2,1],\n",
    "                              padding='SAME')\n",
    "            \n",
    "        in_ch = conv_layer_units[layer_idx]\n",
    "\n",
    "# Top convolution.\n",
    "top_conv = tf.identity(layer, 'top_conv')\n",
    "top_conv_shape = list(map(int, top_conv.get_shape()[1:]))\n",
    "\n",
    "flat_layer = tf.reshape(top_conv, [-1, np.prod(top_conv_shape)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "NkKCAA4SAyHd",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# Fully Connected Layer Archithecture\n",
    "fc_layer_units = [256, 256 ,num_outputs]\n",
    "n_fc_layers = len(fc_layer_units)\n",
    "layer = flat_layer\n",
    "in_ch= int(flat_layer.get_shape()[-1])\n",
    "\n",
    "for layer_idx in range(n_fc_layers):\n",
    "    with tf.variable_scope('fc_{}'.format(layer_idx)):\n",
    "        \n",
    "        # output channels\n",
    "        out_ch = fc_layer_units[layer_idx]\n",
    "        \n",
    "        # kernel \n",
    "        w = tf.Variable(tf.random.normal([in_ch, out_ch], \n",
    "                                              stddev=np.sqrt(2/in_ch)),\n",
    "                             name='weight')\n",
    "\n",
    "        # 추후 weights 만 꺼내 사용하기 위해 weights 을 따로 보관함.          \n",
    "        tf.add_to_collection('weights', w)\n",
    "        \n",
    "        # bias \n",
    "        b = tf.Variable(tf.zeros([out_ch]), name='bias')\n",
    "\n",
    "        # linear transformation \n",
    "        layer = tf.matmul(layer, w) + b\n",
    "\n",
    "        # 마지막 층 , 즉 logit 이 아닐때 적용되는 것들.\n",
    "        if not layer_idx == n_layers-1:\n",
    "            # Dropout\n",
    "            # layer = tf.nn.dropout(layer, drop_rate)\n",
    "\n",
    "            # Activation\n",
    "            layer = tf.nn.relu(layer)\n",
    "            \n",
    "            in_ch = out_ch\n",
    "        \n",
    "logits = layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSr4538goC7W",
    "colab_type": "text"
   },
   "source": [
    "## LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "4CyhshyNl4eM",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# weights 만 가져옵니다. \n",
    "weights = tf.get_collection('weights')\n",
    "\n",
    "# CEE loss\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels)\n",
    "\n",
    "# L1 Regularization : 0.5 * lambda *  ∑(weight)\n",
    "l1_loss = tf.add_n([tf.reduce_sum(weight) for weight in weights])*l1_lambda*0.5\n",
    "\n",
    "# L2 Regularization : 0.5 * lambda * ∑(weight**2)\n",
    "l2_loss = tf.add_n([tf.reduce_sum(weight**2) for weight in weights])*l2_lambda*0.5\n",
    "\n",
    "# CEE + L1 or L2 Regularization\n",
    "total_loss = tf.reduce_mean(loss) + l1_loss + l2_loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45BsTCbOmg1l",
    "colab_type": "text"
   },
   "source": [
    "## Train op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "QBvlXF1KmkN5",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "## high level api을 이용해 Gradient Descent 구현\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6GYOBE4oETP",
    "colab_type": "text"
   },
   "source": [
    "## next batch function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "xISpx1Stner-",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# 복원 추출 합니다. \n",
    "import random\n",
    "def next_batch(xs, ys, batch_size):\n",
    "    indices = random.sample(range(len(ys)), batch_size)\n",
    "    return xs[indices], ys[indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijhDLH38pU0G",
    "colab_type": "text"
   },
   "source": [
    "## Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "VZBTnlf1slyP",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "logits_cls = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "acc = tf.reduce_mean(tf.cast(tf.equal(logits_cls, labels_cls), \n",
    "                     dtype=tf.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6_kPJXzVSRz",
    "colab_type": "text"
   },
   "source": [
    "## Hyperparam 설정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "R0j6oBKBVY_a",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# batch size \n",
    "batch_size = 120 \n",
    "\n",
    "#l2 loss decay\n",
    "l2_decay = 0.0\n",
    "\n",
    "#l1 loss decay\n",
    "l1_decay = 0.0\n",
    "\n",
    "#drop rate \n",
    "# 만약 0.1 이라면 0.1 의 확률로 unit 의 값을 0으로 만듭니다. \n",
    "drop_prob = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jB6ptx6ooW1c",
    "colab_type": "text"
   },
   "source": [
    "## Session Open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "rf2miWhBoQx4",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRQxo6aNnrtX",
    "colab_type": "text"
   },
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "0-gFoZ5IoIq4",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "max_iter = 20000 \n",
    "mean_gradients=[]\n",
    "accum_train_acc = [] \n",
    "accum_train_loss = [] \n",
    "accum_test_acc = [] \n",
    "accum_test_loss = [] \n",
    "\n",
    "for i in range(max_iter):\n",
    "    # random select input data, labels\n",
    "    batch_xs, batch_ys = next_batch(train_images, train_labels, batch_size)\n",
    "\n",
    "    # Training \n",
    "    train_acc, train_loss, _ = sess.run([acc, total_loss, train_op], \n",
    "                                        feed_dict={x: batch_xs,\n",
    "                                                   labels_cls: batch_ys,\n",
    "                                                   learning_rate:0.1,                                                   \n",
    "                                                   l2_lambda:l2_decay,\n",
    "                                                   l1_lambda:l1_decay,\n",
    "                                                   drop_rate:drop_prob,\n",
    "                                                  })\n",
    "    # Evaluating\n",
    "    if i % 100 == 0:\n",
    "        test_acc, test_loss = sess.run([acc, total_loss], \n",
    "                                    feed_dict={x:test_images,\n",
    "                                               labels_cls: test_labels,\n",
    "                                               drop_rate:1.0,                                               \n",
    "                                              })\n",
    "        print('step : {} train_acc : {:.4f} train_loss : {:.4f} validation acc : {:.4f} validation loss : {:.4f}'.\\\n",
    "              format(i, train_acc, train_loss, test_acc, test_loss))\n",
    "        \n",
    "        # list 에 추출함.\n",
    "        accum_train_acc.append(train_acc)\n",
    "        accum_train_loss.append(train_loss)\n",
    "        accum_test_acc.append(test_acc)        \n",
    "        accum_test_loss.append(test_loss)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3h8WbvEpx8Y",
    "colab_type": "text"
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "lGoYUbMLyiQC",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "plt.plot(range(0,20000,100), accum_train_acc, label='train')\n",
    "plt.plot(range(0,20000,100), accum_test_acc, label='test')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "CUkp2Nlv0odN",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "plt.plot(range(0,20000,100), accum_train_loss, label='train')\n",
    "plt.plot(range(0,20000,100), accum_test_loss, label='test')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "fashion_mnist_convolution.ipynb",
   "provenance": [],
   "private_outputs": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
