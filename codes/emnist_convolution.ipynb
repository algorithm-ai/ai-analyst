{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "emnist_convolution.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks4BKH9BRe4_",
        "colab_type": "text"
      },
      "source": [
        "╔══<i><b>&nbsp;Alai-DeepLearning&nbsp;</b></i>══════════════════════════════════╗\n",
        "###  &nbsp;&nbsp; **✎&nbsp;&nbsp;Week 9_cnn_basis**\n",
        "# Homework 1. emnist 을 cnn 으로 학습시키기\n",
        "\n",
        "\n",
        "### _Objective_\n",
        "1. homework 목적\n",
        "이전에 학습 했던 것을 기반으로 좀 더 모델을 발전시켜 봅니다.\n",
        "모델을 학습시켜 95% 이상 정확도를 가진 분류기를 만들어 봅니다. \n",
        "\n",
        "\n",
        "╚═══════════════════════════════════════════════╝\n",
        "\n",
        "---------\n",
        "\n",
        "# 실행 순서 \n",
        "------\n",
        "+ ### 1. DataDownload \n",
        "+ ### 2. EDA \n",
        " #### 1. train, test pixel 값의 EDA\n",
        " #### 2. train ,test class 별 데이터 갯수 \n",
        "\n",
        "+ ### 3. CNN 구성\n",
        " - **layer 별 학습**\n",
        " - **output ch 갯수 별 학습**\n",
        "    - [16, 32, 64, 128 ,26]\n",
        " - **layer 순서별 kernel size** \n",
        "    - (5,5) (3,3), (3,3), (3,3), (3,3)\n",
        " - **Regularization 적용**\n",
        "    - L2, Regularization : 0.0005\n",
        " - **batch 별 학습**\n",
        "    - 120 \n",
        " - **Optimizer**\n",
        "    - Gradient Descent optimizer \n",
        "\n",
        "+ ### 4. metric 측정 \n",
        " - (tensorboard accuracy, loss step별로 측정)\n",
        "\n",
        "+ ### 5. analysis \n",
        " - 각 층별 weights, bias 평균의 변화 측정 (step 별)*이탤릭체 텍스트*\n",
        "------\n",
        "\n",
        "\n",
        "# 실행방법: \n",
        "\n",
        "- DNN 파트를 CNN으로 변경한 후 다시 최적의 파라미터를 찾습니다.<br>\n",
        "그 후 가장 최적의 hyper parameter 을 찾은 후 optimizer 을 바꿔가며 최적화 해 보세요!\n",
        "\n",
        "# 제출 포맷 및 예\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Xoveb0R7fMhk"
      },
      "source": [
        "### 1. EMNIST DataDownLOAD "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3908ofjxfMhd",
        "colab": {}
      },
      "source": [
        "!pip install emnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb_xpB9MQ5-q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import emnist\n",
        "import matplotlib.pyplot as plt \n",
        "import numpy as np \n",
        "import conv_config\n",
        "\n",
        "cfg = conv_config.config_0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPiRq_2M6E81",
        "colab_type": "text"
      },
      "source": [
        "### Image Load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_guXh6WYQ_Gs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Image Load\n",
        "train_images, train_labels = emnist.extract_training_samples('letters')\n",
        "test_images, test_labels = emnist.extract_test_samples('letters')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYZry10E6IJj",
        "colab_type": "text"
      },
      "source": [
        "### Image Info"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHxetItP6LwK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_h, img_w, img_ch = 28, 28, 1\n",
        "n_classes = 26"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqpZfH91hTWf",
        "colab_type": "text"
      },
      "source": [
        "### Normalize and Normalization checking  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qUP6B8KS3t0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.unique([train_labels],return_counts=True)\n",
        "\n",
        "# Normalization\n",
        "train_images = train_images/ 255.\n",
        "test_images = test_images / 255.\n",
        "\n",
        "# Check Normalization \n",
        "def chk_norm(input_data, min_value=0, max_value=1):\n",
        "    \"\"\"\n",
        "    input_data : Ndarray, \n",
        "    min_value : int, input_data의 모든 element는 min_value 이상의 값을 가져야함 \n",
        "    max_value : int, input_data의 모든 element는 max_value 이하의 값을 가져야함 \n",
        "\n",
        "    description: input_data의 모든 element가 특정 범위(min_value ~ max_value)에 있는지 확인합니다.\n",
        "    \"\"\"\n",
        "    input_data = np.asarray(input_data)\n",
        "    assert np.all(input_data <= max_value) & np.all(input_data >= min_value), 'normalize가 잘못 되었습니다.'\n",
        "\n",
        "\n",
        "chk_norm(train_images)    \n",
        "chk_norm(test_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoBOUBz6EDEz",
        "colab_type": "text"
      },
      "source": [
        "## Data reshape "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rf4NF5FPEFm3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_images=train_images.reshape([-1, img_h, img_w, img_ch])\n",
        "test_images=test_images.reshape([-1, img_h, img_w, img_ch])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EILqz9ejk3tf",
        "colab_type": "text"
      },
      "source": [
        "## Input layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h66szIRgcCa9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf \n",
        "num_inputs = img_h*img_w*img_ch # MNIST Input size\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Data를 받아오는 placeholder\n",
        "x = tf.placeholder(tf.float32, shape=[None, img_h, img_w, img_ch],name='x')\n",
        "\n",
        "#labels shape : [1,3,7,7,4,..]\n",
        "labels_cls = tf.placeholder(tf.int32, shape=[None], name='labels') \n",
        "\n",
        "# scalar 을 onehot-vector 형태로 변환합니다.\n",
        "labels = tf.one_hot(labels_cls, depth=n_classes)\n",
        "\n",
        "# Dropout 시 units이 0으로 될 확률.\n",
        "drop_rate = tf.placeholder(tf.float32, shape=[], name='drop_rate')\n",
        "\n",
        "# Learning Rate\n",
        "learning_rate = tf.placeholder_with_default(0.1,shape=(), name='learning_rate')\n",
        "\n",
        "# TODO: shape 을 고정하지 않고 하는 방법도 생각해 보세요\n",
        "l2_lambda = tf.placeholder_with_default(0.0, shape=[], name='l2_lambda') \n",
        "l1_lambda = tf.placeholder_with_default(0.0, shape=[], name='l1_lambda')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDJ5cNp7pSrU",
        "colab_type": "text"
      },
      "source": [
        "# BUILD MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAmNm5S0Epwr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CNN Archithecture\n",
        "# layer 순서별 kernel_size \n",
        "kernel_sizes = cfg.conv_kernels\n",
        "conv_units = cfg.conv_units\n",
        "pooling_schedule = cfg.pooling_schedule\n",
        "\n",
        "# layer 길이 \n",
        "n_layers = len(conv_units)\n",
        "\n",
        "\n",
        "# conv layer 구축\n",
        "in_ch = img_ch\n",
        "layer = x \n",
        "for layer_idx in range(n_layers):\n",
        "    with tf.variable_scope('conv_{}'.format(layer_idx)):\n",
        "\n",
        "        # output channels\n",
        "        out_ch = conv_units[layer_idx]\n",
        "        \n",
        "        # kernel \n",
        "        k_h, k_w = kernel_sizes[layer_idx]\n",
        "        kernel = tf.Variable(tf.random.normal([k_h, k_w, in_ch, out_ch],\n",
        "                                              stddev=0.1), name='weight')\n",
        "\n",
        "        # 추후 weights 만 꺼내 사용하기 위해 weights 을 따로 보관함.          \n",
        "        tf.add_to_collection('weights', kernel)\n",
        "        \n",
        "        # Bias \n",
        "        b = tf.Variable(tf.zeros([out_ch]), name='bias')\n",
        "\n",
        "        # Convolution\n",
        "        layer = tf.nn.conv2d(layer, kernel, strides=[1, 1, 1, 1], padding='SAME') + b\n",
        "        \n",
        "        # Activation\n",
        "        layer = tf.nn.relu(layer)\n",
        "\n",
        "        # pooling 을 적용하면 input tensor 의 h,w 을 1/2사이즈로 줄입니다. \n",
        "        if pooling_schedule[layer_idx]:\n",
        "            layer = tf.nn.max_pool(layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
        "                                padding='SAME')\n",
        "                \n",
        "        in_ch = conv_units[layer_idx]\n",
        "        \n",
        "        # layer_info\n",
        "        layer_info = in_ch, out_ch, k_h, k_w, pooling_schedule[layer_idx], layer.get_shape()\n",
        "        print(layer_info)\n",
        "\n",
        "# Top convolution.\n",
        "top_conv = tf.identity(layer, 'top_conv')\n",
        "top_conv_shape = list(map(int, top_conv.get_shape()[1:]))\n",
        "\n",
        "flat_layer = tf.reshape(top_conv, [-1, np.prod(top_conv_shape)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkKCAA4SAyHd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fully Connected Layer Archithecture\n",
        "fc_units = cfg.fc_units\n",
        "n_fc_layers = len(fc_units)\n",
        "layer = flat_layer\n",
        "in_ch= int(flat_layer.get_shape()[-1])\n",
        "\n",
        "for layer_idx in range(n_fc_layers):\n",
        "    with tf.variable_scope('fc_{}'.format(layer_idx)):\n",
        "        \n",
        "        # output channels\n",
        "        out_ch = fc_units[layer_idx]\n",
        "        \n",
        "        # kernel \n",
        "        w = tf.Variable(tf.random.normal([in_ch, out_ch], \n",
        "                                              stddev=np.sqrt(2/in_ch)),\n",
        "                             name='weight')\n",
        "\n",
        "        # 추후 weights 만 꺼내 사용하기 위해 weights 을 따로 보관함.          \n",
        "        tf.add_to_collection('weights', w)\n",
        "        \n",
        "        # bias \n",
        "        b = tf.Variable(tf.zeros([out_ch]), name='bias')\n",
        "\n",
        "        # linear transformation \n",
        "        layer = tf.matmul(layer, w) + b\n",
        "        \n",
        "        # 마지막 층 , 즉 logit 이 아닐때 적용되는 것들.\n",
        "        if not layer_idx == n_layers-1:\n",
        "            #Dropout\n",
        "            layer = tf.nn.dropout(layer, rate=drop_rate)\n",
        "\n",
        "            # Activation\n",
        "            layer = tf.nn.relu(layer)\n",
        "            \n",
        "            in_ch = out_ch\n",
        "        # layer info \n",
        "        print('units : {}, w shape : {}, b shape :{}'.format(out_ch, w.shape, b.shape))\n",
        "\n",
        "logits = layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSr4538goC7W",
        "colab_type": "text"
      },
      "source": [
        "## LOSS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CyhshyNl4eM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# weights 만 가져옵니다. \n",
        "weights = tf.get_collection('weights')\n",
        "for w in weights:\n",
        "    print(w.get_shape)\n",
        "\n",
        "# CEE loss\n",
        "loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels)\n",
        "\n",
        "# L1 Regularization : 0.5 * lambda *  ∑(weight)\n",
        "l1_loss = tf.add_n([tf.reduce_sum(tf.abs(weight)) for weight in weights])*l1_lambda*0.5\n",
        "\n",
        "# L2 Regularization : 0.5 * lambda * ∑(weight**2)\n",
        "l2_loss = tf.add_n([tf.reduce_sum(weight**2) for weight in weights])*l2_lambda*0.5\n",
        "\n",
        "# CEE + L1 or L2 Regularization\n",
        "total_loss = tf.reduce_mean(loss) + l1_loss + l2_loss "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45BsTCbOmg1l",
        "colab_type": "text"
      },
      "source": [
        "## Train op"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBvlXF1KmkN5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if cfg.optimizer == 'adam':\n",
        "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n",
        "elif cfg.optimizer == 'gd':\n",
        "    train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)\n",
        "elif cfg.optimizer == 'rmsp':\n",
        "    train_op = tf.train.RMSPropOptimizer(learning_rate).minimize(total_loss)\n",
        "elif cfg.optimizer == 'momentum':\n",
        "    train_op = tf.train.Momentum(learning_rate, momentum=0.999).minimize(total_loss)\n",
        "else:\n",
        "    raise NotImplementedError\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6GYOBE4oETP",
        "colab_type": "text"
      },
      "source": [
        "## next batch function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xISpx1Stner-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 복원 추출 합니다. \n",
        "import random\n",
        "def next_batch(xs, ys, batch_size):\n",
        "    indices = random.sample(range(len(ys)), batch_size)\n",
        "    return xs[indices], ys[indices]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijhDLH38pU0G",
        "colab_type": "text"
      },
      "source": [
        "## Metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZBTnlf1slyP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logits_cls = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
        "acc = tf.reduce_mean(tf.cast(tf.equal(logits_cls, labels_cls), \n",
        "                     dtype=tf.float32))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6_kPJXzVSRz",
        "colab_type": "text"
      },
      "source": [
        "## Hyperparam 설정\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0j6oBKBVY_a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# batch size \n",
        "batch_size = cfg.batch_size\n",
        "\n",
        "#l2 loss decay\n",
        "l2_decay = cfg.l2_lambda\n",
        "\n",
        "#l1 loss decay\n",
        "l1_decay = cfg.l1_lambda\n",
        "\n",
        "#drop rate \n",
        "# 만약 0.1 이라면 0.1 의 확률로 unit 의 값을 0으로 만듭니다. \n",
        "drop_prob = cfg.drop_rate\n",
        "\n",
        "# learning rate \n",
        "lr = cfg.lr\n",
        "\n",
        "# max iteration \n",
        "max_iter = cfg.max_iter\n",
        "ckpt = cfg.ckpt\n",
        "print('batch_size : {} l2 : {} l1 : {} drop_rate : {} lr : {}'.format(batch_size, l2_decay, l1_decay, drop_prob, lr))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jB6ptx6ooW1c",
        "colab_type": "text"
      },
      "source": [
        "## Session Open"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rf2miWhBoQx4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess = tf.Session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRQxo6aNnrtX",
        "colab_type": "text"
      },
      "source": [
        "## Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-gFoZ5IoIq4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess.run(tf.global_variables_initializer())\n",
        "max_iter = 100000 \n",
        "mean_gradients=[]\n",
        "accum_train_acc = [] \n",
        "accum_train_loss = [] \n",
        "accum_test_acc = [] \n",
        "accum_test_loss = [] \n",
        "\n",
        "for i in range(max_iter):\n",
        "    # random select input data, labels\n",
        "    batch_xs, batch_ys = next_batch(train_images, train_labels, batch_size)\n",
        "    # Training \n",
        "    train_acc, train_loss, _ = sess.run([acc, total_loss, train_op], \n",
        "                                        feed_dict={x: batch_xs,\n",
        "                                                   labels_cls: batch_ys,\n",
        "                                                   learning_rate: lr,                                                   \n",
        "                                                   l2_lambda:l2_decay,\n",
        "                                                   l1_lambda:l1_decay,\n",
        "                                                   drop_rate:drop_prob,\n",
        "                                                  })\n",
        "    # Evaluating\n",
        "    if i % 100 == 0:\n",
        "        test_acc, test_loss = sess.run([acc, total_loss], \n",
        "                                    feed_dict={x:test_images,\n",
        "                                               labels_cls: test_labels,\n",
        "                                               drop_rate:0.0,                                               \n",
        "                                              })\n",
        "        print('step : {} train_acc : {:.4f} train_loss : {:.4f} validation acc : {:.4f} validation loss : {:.4f}'.\\\n",
        "              format(i, train_acc, train_loss, test_acc, test_loss))\n",
        "        \n",
        "        # list 에 추출함.\n",
        "        accum_train_acc.append(train_acc)\n",
        "        accum_train_loss.append(train_loss)\n",
        "        accum_test_acc.append(test_acc)        \n",
        "        accum_test_loss.append(test_loss)        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3h8WbvEpx8Y",
        "colab_type": "text"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGoYUbMLyiQC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(range(0,max_iter, ckpt), accum_train_acc, label='train')\n",
        "plt.plot(range(0,max_iter, ckpt), accum_test_acc, label='test')\n",
        "plt.title('Accuracy')\n",
        "plt.legend()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUkp2Nlv0odN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(range(0,max_iter, ckpt), accum_train_loss, label='train')\n",
        "plt.plot(range(0,max_iter, ckpt), accum_test_loss, label='test')\n",
        "plt.title('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}