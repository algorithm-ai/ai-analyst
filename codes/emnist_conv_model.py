# -*- coding: utf-8 -*-
"""emnist_conv_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DZXFi4FJyHpDz1tKcb22sUiNw_OpAjz-
"""

import emnist
import matplotlib.pyplot as plt 
import numpy as np
import argparse 
import conv_config as config
import os 
import sys

parser = argparse.ArgumentParser()       
parser.add_argument('--config_num')
args = parser.parse_args()
cfg_num = args.config_num
cfg = config.config_dict[cfg_num]

# Data Load 
train_images, train_labels = emnist.extract_training_samples('letters')
test_images, test_labels = emnist.extract_test_samples('letters')

# Data info 
img_h, img_w, img_ch = 28, 28, 1
n_classes = 26

# Normalization
train_images = train_images/ 255.
test_images = test_images / 255.

# Image reshape for conv 
train_images=train_images.reshape([-1, img_h, img_w, img_ch])
test_images=test_images.reshape([-1, img_h, img_w, img_ch])

"""
Description:
    input tensor 
"""

import tensorflow as tf 
num_inputs = img_h*img_w*img_ch # MNIST Input size

tf.reset_default_graph()

# Data를 받아오는 placeholder
x = tf.placeholder(tf.float32, shape=[None, img_h, img_w, img_ch],name='x')

#labels shape : [1,3,7,7,4,..]
labels_cls = tf.placeholder(tf.int32, shape=[None], name='labels') 

# scalar 을 onehot-vector 형태로 변환합니다.
labels = tf.one_hot(labels_cls, depth=n_classes)

# Dropout 시 units이 0으로 될 확률.
drop_rate = tf.placeholder(tf.float32, shape=[], name='drop_rate')

# Learning Rate
learning_rate = tf.placeholder_with_default(0.1,shape=(), name='learning_rate')

# TODO: shape 을 고정하지 않고 하는 방법도 생각해 보세요
l2_lambda = tf.placeholder_with_default(0.0, shape=[], name='l2_lambda') 
l1_lambda = tf.placeholder_with_default(0.0, shape=[], name='l1_lambda')

"""
Description:
    CNN Archithecture 정의
"""
kernel_sizes = cfg.conv_kernels
conv_units = cfg.conv_units
pooling_schedule = cfg.pooling_schedule

# layer 길이 
n_layers = len(conv_units)


# conv layer 구축
in_ch = img_ch
layer = x 
for layer_idx in range(n_layers):
    with tf.variable_scope('conv_{}'.format(layer_idx)):

        # output channels
        out_ch = conv_units[layer_idx]
        
        # kernel size 
        k_h, k_w = kernel_sizes[layer_idx]
        kernel = tf.Variable(tf.random.normal([k_h, k_w, in_ch, out_ch],
                                              stddev=0.1), name='weight')

        # 추후 weights 만 꺼내 사용하기 위해 weights 을 따로 보관함.          
        tf.add_to_collection('weights', kernel)
        
        # Bias 
        b = tf.Variable(tf.zeros([out_ch]), name='bias')

        # Convolution
        layer = tf.nn.conv2d(layer, kernel, strides=[1, 1, 1, 1], padding='SAME') + b
        
        # Activation
        layer = tf.nn.relu(layer)

        # pooling 을 적용하면 input tensor 의 h,w 을 1/2사이즈로 줄입니다. 
        if pooling_schedule[layer_idx]:
            layer = tf.nn.max_pool(layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],
                                padding='SAME')
                
        in_ch = conv_units[layer_idx]
        
        # layer_info
        layer_info = in_ch, out_ch, k_h, k_w, pooling_schedule[layer_idx], layer.get_shape()
        print(layer_info)

# Top convolution.
top_conv = tf.identity(layer, 'top_conv')
top_conv_shape = list(map(int, top_conv.get_shape()[1:]))

flat_layer = tf.reshape(top_conv, [-1, np.prod(top_conv_shape)])

# Fully Connected Layer Archithecture
fc_units = cfg.fc_units
n_fc_layers = len(fc_units)
layer = flat_layer
in_ch= int(flat_layer.get_shape()[-1])

for layer_idx in range(n_fc_layers):
    with tf.variable_scope('fc_{}'.format(layer_idx)):
        
        # output channels
        out_ch = fc_units[layer_idx]
        
        # kernel 생성
        w = tf.Variable(tf.random.normal([in_ch, out_ch], 
                                              stddev=np.sqrt(2/in_ch)),
                             name='weight')

        # 추후 weights 만 꺼내 사용하기 위해 weights 을 따로 보관함.          
        tf.add_to_collection('weights', w)
        
        # bias 생성
        b = tf.Variable(tf.zeros([out_ch]), name='bias')

        # linear transformation 
        layer = tf.matmul(layer, w) + b
        
        # 마지막 층 , 즉 logit 이 아닐때 적용되는 것들.
        if not layer_idx == n_layers-1:
            #Dropout
            layer = tf.nn.dropout(layer, rate=drop_rate)
            # Activation
            layer = tf.nn.relu(layer)
            # 
            in_ch = out_ch
        # layer info 
        print('units : {}, w shape : {}, b shape :{}'.format(out_ch, w.shape, b.shape))

logits = layer

# weights 만 가져옵니다. 
weights = tf.get_collection('weights')
for w in weights:
    print(w.get_shape)

# CEE loss
loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels)

# L1 Regularization : 0.5 * lambda *  ∑(weight)
l1_loss = tf.add_n([tf.reduce_sum(tf.abs(weight)) for weight in weights])*l1_lambda*0.5

# L2 Regularization : 0.5 * lambda * ∑(weight**2)
l2_loss = tf.add_n([tf.reduce_sum(weight**2) for weight in weights])*l2_lambda*0.5

# CEE + L1 or L2 Regularization
total_loss = tf.reduce_mean(loss) + l1_loss + l2_loss

if cfg.optimizer == 'adam':
    train_op = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)
elif cfg.optimizer == 'gd':
    train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)
elif cfg.optimizer == 'rmsp':
    train_op = tf.train.RMSPropOptimizer(learning_rate).minimize(total_loss)
elif cfg.optimizer == 'momentum':
    train_op = tf.train.Momentum(learning_rate, momentum=0.999).minimize(total_loss)
else:
    raise NotImplementedError

# 복원 추출 합니다. 
import random
def next_batch(xs, ys, batch_size):
    indices = random.sample(range(len(ys)), batch_size)
    return xs[indices], ys[indices]

logits_cls = tf.argmax(logits, axis=1, output_type=tf.int32)
acc = tf.reduce_mean(tf.cast(tf.equal(logits_cls, labels_cls), 
                     dtype=tf.float32))

# batch size 
batch_size = cfg.batch_size

#l2 loss decay
l2_decay = cfg.l2_lambda

#l1 loss decay
l1_decay = cfg.l1_lambda

#drop rate 
# 만약 0.1 이라면 0.1 의 확률로 unit 의 값을 0으로 만듭니다. 
drop_prob = cfg.drop_rate

# learning rate 
lr = cfg.lr

# max iteration 
max_iter = cfg.max_iter
ckpt = cfg.ckpt
print('batch_size : {} l2 : {} l1 : {} drop_rate : {} lr : {}'.format(batch_size, l2_decay, l1_decay, drop_prob, lr))

#setting
save_dir = cfg_num
os.makedirs(save_dir)
f = open(os.path.join(save_dir, 'log.txt'), 'w')

sess = tf.Session()

sess.run(tf.global_variables_initializer())
max_iter = 100000 
mean_gradients=[]
accum_train_acc = [] 
accum_train_loss = [] 
accum_test_acc = [] 
accum_test_loss = [] 

for i in range(max_iter):
    # 
    sys.stdout.write('\r progress {} {}'.format(i, max_iter))
    sys.stdout.flush()

    # random select input data, labels
    batch_xs, batch_ys = next_batch(train_images, train_labels, batch_size)
    
    # Training 
    train_acc, train_loss, _ = sess.run([acc, total_loss, train_op], 
                                        feed_dict={x: batch_xs,
                                                   labels_cls: batch_ys,
                                                   learning_rate: lr,                                                   
                                                   l2_lambda:l2_decay,
                                                   l1_lambda:l1_decay,
                                                   drop_rate:drop_prob,
                                                  })
    # Evaluating
    if i % 100 == 0:
        test_acc, test_loss = sess.run([acc, total_loss], 
                                    feed_dict={x:test_images,
                                               labels_cls: test_labels,
                                               drop_rate:0.0,                                               
                                              })
        f.write('{}\t{}\t{}\t{}\t{}\n'.format(i, train_acc, train_loss, 
                                              test_acc, test_loss))
        f.flush()
        # list 에 추출함.
        accum_train_acc.append(train_acc)
        accum_train_loss.append(train_loss)
        accum_test_acc.append(test_acc)        
        accum_test_loss.append(test_loss)

