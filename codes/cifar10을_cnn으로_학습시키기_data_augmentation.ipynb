{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ks4BKH9BRe4_"
   },
   "source": [
    "╔══<i><b>&nbsp;Alai-DeepLearning&nbsp;</b></i>══════════════════════════════════╗\n",
    "###  &nbsp;&nbsp; **✎&nbsp;&nbsp;Week 9_cnn_basis**\n",
    "# Homework 3. Cifar10 을 CNN 으로 학습시키기\n",
    "\n",
    "\n",
    "### _Objective_\n",
    "1. homework 목적\n",
    "Color Image 인 Cifar10 데이터를 CNN으로 학습시켜 봅니다.\n",
    "모델을 작성 후 모델을 학습시켜 80% 이상 정확도를 가진 분류기를 만들어 봅니다. \n",
    "\n",
    "\n",
    "╚═══════════════════════════════════════════════╝\n",
    "\n",
    "---------\n",
    "\n",
    "# 실행 순서 \n",
    "------\n",
    "+ ### 1. DataDownload \n",
    "+ ### 2. EDA \n",
    " #### 1. train, test pixel 값의 EDA\n",
    " #### 2. train ,test class 별 데이터 갯수 \n",
    "\n",
    "+ ### 3. CNN 구성\n",
    " - **layer 별 학습**\n",
    " - **output ch 갯수 별 학습**\n",
    "    - [16, 32, 64, 128 ,26]\n",
    " - **layer 순서별 kernel size** \n",
    "    - (5,5) (3,3), (3,3), (3,3), (3,3)\n",
    " - **Regularization 적용**\n",
    "    - L2, Regularization : 0.0005\n",
    " - **batch 별 학습**\n",
    "    - 120 \n",
    " - **Optimizer**\n",
    "    - Gradient Descent optimizer \n",
    "\n",
    "+ ### 4. metric 측정 \n",
    " - (tensorboard accuracy, loss step별로 측정)\n",
    "\n",
    "+ ### 5. analysis \n",
    " - 각 층별 weights, bias 평균의 변화 측정 (step 별)*이탤릭체 텍스트*\n",
    "------\n",
    "\n",
    "\n",
    "# 실행방법: \n",
    "\n",
    "- DNN 파트를 CNN으로 변경한 후 다시 최적의 파라미터를 찾습니다.<br>\n",
    "그 후 가장 최적의 hyper parameter 을 찾은 후 optimizer 을 바꿔가며 최적화 해 보세요!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sZxGJKYeeQ9_"
   },
   "source": [
    "### 1. DataDownLOAD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tb_xpB9MQ5-q"
   },
   "outputs": [],
   "source": [
    "import keras \n",
    "cifar10 = keras.datasets.cifar10\n",
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kQ-hEcJURVJK"
   },
   "outputs": [],
   "source": [
    "# 이미지 데이터 셋 sample \n",
    "plt.imshow(train_images[0])\n",
    "plt.show()\n",
    "# 라벨 데이터 셋 sample \n",
    "print('label sample :', train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2TgDylTThGF4"
   },
   "source": [
    "#### Image EDA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cg1_muqTg-XX"
   },
   "source": [
    "1. train 최대값 최소값 분포도 / 평균, 분산 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F4rXGPknhSwr"
   },
   "outputs": [],
   "source": [
    "# Train Image \n",
    "n_train = len(train_images)\n",
    "seq_length = np.prod(train_images.shape[1:])\n",
    "h, w, ch = train_images.shape[1:]\n",
    "\n",
    "# 모든 이미지의 min, max 값 확인\n",
    "train_min = train_images.reshape([-1,seq_length]).min(axis=1)\n",
    "train_max = train_images.reshape([-1,seq_length]).max(axis=1)\n",
    "\n",
    "train_mean = train_images.reshape([-1,seq_length]).mean(axis=1)\n",
    "train_std = train_images.reshape([-1,seq_length]).std(axis=1)\n",
    "\n",
    "print('# train : {}'.format(n_train))\n",
    "print('seq_length : {}'.format(seq_length))\n",
    "print('h, w, ch : {} {} {}'.format(h, w, ch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YxJIXlDGkWkG"
   },
   "outputs": [],
   "source": [
    "# Test Image \n",
    "n_test = len(test_images)\n",
    "seq_length = np.prod(test_images.shape[1:])\n",
    "\n",
    "test_min = test_images.reshape([-1,seq_length]).min(axis=1)\n",
    "test_max = test_images.reshape([-1,seq_length]).max(axis=1)\n",
    "\n",
    "test_mean = test_images.reshape([-1,seq_length]).mean(axis=1)\n",
    "test_std = test_images.reshape([-1,seq_length]).std(axis=1)\n",
    "\n",
    "print('# test : {}'.format(n_test))\n",
    "print('seq_length : {}'.format(seq_length))\n",
    "print('h, w, ch : {} {} {}'.format(h, w, ch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lq14-GpxgXUQ"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(train_min, train_max, s=3, alpha=0.5, label='train')\n",
    "plt.scatter(test_min, test_max, s=3, alpha=0.5, label='test')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.hist(train_mean, alpha=0.3, label='train')\n",
    "plt.hist(test_mean, alpha=0.3, label='test')\n",
    "plt.title('Mean')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.hist(train_std, alpha=0.3, label='train std')\n",
    "plt.hist(test_std, alpha=0.3, label='test std')\n",
    "plt.title('Stddev')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hox-JI_akSfw"
   },
   "source": [
    "#### Label EDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K47CeLpSjBsB"
   },
   "outputs": [],
   "source": [
    "print('train label data shape : {}'.format(train_labels.shape))\n",
    "print('test label data shape : {}'.format(test_labels.shape))\n",
    "print('label 을 scalar 형태로 reshape 합니다 ex){5000,1} -> {5000}')\n",
    "train_labels = train_labels.reshape(-1)\n",
    "test_labels = test_labels.reshape(-1)\n",
    "print('train label data shape : {}'.format(train_labels.shape))\n",
    "print('test label data shape : {}'.format(test_labels.shape))\n",
    "\n",
    "print(np.unique(train_labels, return_counts=True))\n",
    "print(np.unique(test_labels, return_counts=True))\n",
    "\n",
    "num_outputs = len(np.unique(test_labels, return_counts=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fqpZfH91hTWf"
   },
   "source": [
    "### Normalize and Normalization checking  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-qUP6B8KS3t0"
   },
   "outputs": [],
   "source": [
    "np.unique([train_labels],return_counts=True)\n",
    "\n",
    "# Normalization\n",
    "train_images = train_images.reshape(-1,seq_length) / 255.\n",
    "test_images = test_images.reshape(-1,seq_length) / 255.\n",
    "\n",
    "# Check Normalization \n",
    "def chk_norm(input_data, min_value=0, max_value=1):\n",
    "    \"\"\"\n",
    "    input_data : Ndarray, \n",
    "    min_value : int, input_data의 모든 element는 min_value 이상의 값을 가져야함 \n",
    "    max_value : int, input_data의 모든 element는 max_value 이하의 값을 가져야함 \n",
    "\n",
    "    description: input_data의 모든 element가 특정 범위(min_value ~ max_value)에 있는지 확인합니다.\n",
    "    \"\"\"\n",
    "    input_data = np.asarray(input_data)\n",
    "    assert np.all(input_data <= max_value) & np.all(input_data >= min_value), 'normalize가 잘못 되었습니다.'\n",
    "\n",
    "\n",
    "chk_norm(train_images)    \n",
    "chk_norm(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PoBOUBz6EDEz"
   },
   "source": [
    "## Data reshape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rf4NF5FPEFm3"
   },
   "outputs": [],
   "source": [
    "train_images=train_images.reshape([-1, h, w, ch])\n",
    "test_images=test_images.reshape([-1, h, w, ch])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EILqz9ejk3tf"
   },
   "source": [
    "## Input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h66szIRgcCa9"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "num_inputs = h*w # MNIST Input size\n",
    "num_outputs = 10 # The number of Label : 26\n",
    "image_ch = ch\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Data를 받아오는 placeholder\n",
    "x = tf.placeholder(tf.float32, shape=[None, h, w, image_ch],name='x')\n",
    "\n",
    "#labels shape : [1,3,7,7,4,..]\n",
    "labels_cls = tf.placeholder(tf.int32, shape=[None], name='labels') \n",
    "\n",
    "# scalar 을 onehot-vector 형태로 변환합니다.\n",
    "labels = tf.one_hot(labels_cls, depth=num_outputs)\n",
    "\n",
    "# Dropout 시 units이 0으로 될 확률.\n",
    "drop_rate = tf.placeholder(tf.float32, shape=[], name='drop_rate')\n",
    "\n",
    "# Learning Rate\n",
    "learning_rate = tf.placeholder_with_default(0.1,shape=(), name='learning_rate')\n",
    "\n",
    "# 학습 flag \n",
    "phase_train = tf.placeholder(tf.bool, shape=[], name='phase_train')\n",
    "\n",
    "# TODO: shape 을 고정하지 않고 하는 방법도 생각해 보세요\n",
    "num_layers=5\n",
    "l2_lambda = tf.placeholder_with_default(0.0, shape=[], name='l2_lambda') \n",
    "l1_lambda = tf.placeholder_with_default(0.0, shape=[], name='l1_lambda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GDJ5cNp7pSrU"
   },
   "source": [
    "# BUILD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aAmNm5S0Epwr"
   },
   "outputs": [],
   "source": [
    "# CNN Archithecture\n",
    "# layer 순서별 kernel_size \n",
    "kernel_sizes = [(5,5),(3,3),(3,3),(3,3)]\n",
    "conv_layer_units = [16, 32, 64, 128]\n",
    "\n",
    "# layer 길이 \n",
    "n_layers = len(conv_layer_units)\n",
    "\n",
    "#augmentation \n",
    "def train_aug(img):\n",
    "    img=tf.image.random_brightness(img, 0.01)\n",
    "    #img=tf.image.random_contrast(img, 0, 0.1)\n",
    "    img=tf.image.random_flip_left_right(img)\n",
    "    img=tf.image.random_flip_up_down(img)\n",
    "    img=tf.image.random_saturation(img, lower=0, upper=0.1)\n",
    "    img=tf.image.random_hue(img, 0.1)\n",
    "    \n",
    "    return img\n",
    "\n",
    "def true_fn():\n",
    "    return tf.map_fn(train_aug, x)\n",
    "\n",
    "def false_fn():\n",
    "    return x\n",
    "\n",
    "aug_x = tf.cond(phase_train , true_fn, false_fn)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# conv layer 구축\n",
    "in_ch = image_ch\n",
    "layer = aug_x\n",
    "\n",
    "\n",
    "\n",
    "for layer_idx in range(n_layers):\n",
    "    with tf.variable_scope('conv_{}'.format(layer_idx)):\n",
    "\n",
    "        # output channels\n",
    "        out_ch = conv_layer_units[layer_idx]\n",
    "        \n",
    "        # kernel \n",
    "        k_h, k_w = kernel_sizes[layer_idx]\n",
    "        kernel = tf.Variable(tf.random.normal([k_h, k_w, in_ch, out_ch],\n",
    "                                              stddev=np.sqrt(2/num_inputs)),\n",
    "                             name='weight')\n",
    "\n",
    "        # 추후 weights 만 꺼내 사용하기 위해 weights 을 따로 보관함.          \n",
    "        tf.add_to_collection('weights', kernel)\n",
    "        \n",
    "        # bias \n",
    "        b = tf.Variable(tf.zeros([out_ch]), name='bias')\n",
    "\n",
    "        # Convolution\n",
    "        layer = tf.nn.conv2d(layer, kernel, strides=[1,1,1,1], padding='SAME') + b\n",
    "        \n",
    "        # Activation\n",
    "        layer = tf.nn.relu(layer)\n",
    "        \n",
    "        # max pooling : 이미지를 1/2 로 줄입니다. \n",
    "        layer = tf.nn.max_pool(layer, ksize=[1,2,2,1], strides=[1,2,2,1],\n",
    "                              padding='SAME')\n",
    "            \n",
    "        in_ch = conv_layer_units[layer_idx]\n",
    "\n",
    "# Top convolution.\n",
    "top_conv = tf.identity(layer, 'top_conv')\n",
    "top_conv_shape = list(map(int, top_conv.get_shape()[1:]))\n",
    "\n",
    "flat_layer = tf.reshape(top_conv, [-1, np.prod(top_conv_shape)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NkKCAA4SAyHd"
   },
   "outputs": [],
   "source": [
    "# Fully Connected Layer Archithecture\n",
    "fc_layer_units = [256, 256 ,num_outputs]\n",
    "n_fc_layers = len(fc_layer_units)\n",
    "layer = flat_layer\n",
    "in_ch= int(flat_layer.get_shape()[-1])\n",
    "\n",
    "for layer_idx in range(n_fc_layers):\n",
    "    with tf.variable_scope('fc_{}'.format(layer_idx)):\n",
    "        \n",
    "        # output channels\n",
    "        out_ch = fc_layer_units[layer_idx]\n",
    "        \n",
    "        # kernel \n",
    "        w = tf.Variable(tf.random.normal([in_ch, out_ch], \n",
    "                                              stddev=np.sqrt(2/in_ch)),\n",
    "                             name='weight')\n",
    "        # 추후 weights 만 꺼내 사용하기 위해 weights 을 따로 보관함.          \n",
    "        tf.add_to_collection('weights', w)\n",
    "        \n",
    "        # bias \n",
    "        b = tf.Variable(tf.zeros([out_ch]), name='bias')\n",
    "\n",
    "        # linear transformation \n",
    "        layer = tf.matmul(layer, w) + b\n",
    "\n",
    "        # 마지막 층 , 즉 logit 이 아닐때 적용되는 것들.\n",
    "        if not layer_idx == n_layers-1:\n",
    "            # Dropout\n",
    "            layer = tf.nn.dropout(layer, rate=drop_rate)\n",
    "\n",
    "            # Activation\n",
    "            layer = tf.nn.relu(layer)\n",
    "            \n",
    "            in_ch = out_ch\n",
    "        \n",
    "logits = layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VSr4538goC7W"
   },
   "source": [
    "## LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4CyhshyNl4eM"
   },
   "outputs": [],
   "source": [
    "# weights 만 가져옵니다. \n",
    "weights = tf.get_collection('weights')\n",
    "\n",
    "# CEE loss\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels)\n",
    "\n",
    "# L1 Regularization : 0.5 * lambda *  ∑(weight)\n",
    "l1_loss = tf.add_n([tf.reduce_sum(tf.abs(weight)) for weight in weights])*l1_lambda*0.5\n",
    "\n",
    "# L2 Regularization : 0.5 * lambda * ∑(weight**2)\n",
    "l2_loss = tf.add_n([tf.reduce_sum(weight**2) for weight in weights])*l2_lambda*0.5\n",
    "\n",
    "# CEE + L1 or L2 Regularization\n",
    "total_loss = tf.reduce_mean(loss) + l1_loss + l2_loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "45BsTCbOmg1l"
   },
   "source": [
    "## Train op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QBvlXF1KmkN5"
   },
   "outputs": [],
   "source": [
    "## high level api을 이용해 Gradient Descent 구현\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P6GYOBE4oETP"
   },
   "source": [
    "## next batch function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xISpx1Stner-"
   },
   "outputs": [],
   "source": [
    "# 복원 추출 합니다. \n",
    "import random\n",
    "def next_batch(xs, ys, batch_size):\n",
    "    indices = random.sample(range(len(ys)), batch_size)\n",
    "    return xs[indices], ys[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ijhDLH38pU0G"
   },
   "source": [
    "## Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VZBTnlf1slyP"
   },
   "outputs": [],
   "source": [
    "logits_cls = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "acc = tf.reduce_mean(tf.cast(tf.equal(logits_cls, labels_cls), \n",
    "                     dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f6_kPJXzVSRz"
   },
   "source": [
    "## Hyperparam 설정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R0j6oBKBVY_a"
   },
   "outputs": [],
   "source": [
    "# batch size \n",
    "batch_size = 120 \n",
    "\n",
    "#l2 loss decay\n",
    "l2_decay = 0.0\n",
    "\n",
    "#l1 loss decay\n",
    "l1_decay = 0.0\n",
    "\n",
    "#drop rate \n",
    "# 만약 0.1 이라면 0.1 의 확률로 unit 의 값을 0으로 만듭니다. \n",
    "drop_prob = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jB6ptx6ooW1c"
   },
   "source": [
    "## Session Open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rf2miWhBoQx4"
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WRQxo6aNnrtX"
   },
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0-gFoZ5IoIq4"
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "max_iter = 40000 \n",
    "ckpt = 1000\n",
    "mean_gradients=[]\n",
    "accum_train_acc = [] \n",
    "accum_train_loss = [] \n",
    "accum_test_acc = [] \n",
    "accum_test_loss = [] \n",
    "\n",
    "for i in range(max_iter):\n",
    "    # random select input data, labels\n",
    "    batch_xs, batch_ys = next_batch(train_images, train_labels, batch_size)\n",
    "\n",
    "    # Training \n",
    "    train_acc, train_loss, _ = sess.run([acc, total_loss, train_op], \n",
    "                                        feed_dict={x: batch_xs,\n",
    "                                                   labels_cls: batch_ys,\n",
    "                                                   learning_rate:0.1,                                                   \n",
    "                                                   l2_lambda:l2_decay,\n",
    "                                                   l1_lambda:l1_decay,\n",
    "                                                   drop_rate:drop_prob,\n",
    "                                                   phase_train:True\n",
    "                                                  })\n",
    "    # Evaluating\n",
    "    if i % ckpt == 0:\n",
    "        test_acc, test_loss = sess.run([acc, total_loss], \n",
    "                                    feed_dict={x:test_images,\n",
    "                                               labels_cls: test_labels,\n",
    "                                               drop_rate:0.0,\n",
    "                                               phase_train: False\n",
    "                                              })\n",
    "        print('step : {} train_acc : {:.4f} train_loss : {:.4f} validation acc : {:.4f} validation loss : {:.4f}'.\\\n",
    "              format(i, train_acc, train_loss, test_acc, test_loss))\n",
    "        \n",
    "        # list 에 추출함.\n",
    "        accum_train_acc.append(train_acc)\n",
    "        accum_train_loss.append(train_loss)\n",
    "        accum_test_acc.append(test_acc)        \n",
    "        accum_test_loss.append(test_loss)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p3h8WbvEpx8Y"
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lGoYUbMLyiQC"
   },
   "outputs": [],
   "source": [
    "plt.plot(range(0, max_iter, ckpt), accum_train_acc, label='train')\n",
    "plt.plot(range(0, max_iter, ckpt), accum_test_acc, label='test')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CUkp2Nlv0odN"
   },
   "outputs": [],
   "source": [
    "plt.plot(range(0,max_iter, ckpt), accum_train_loss, label='train')\n",
    "plt.plot(range(0,max_iter, ckpt), accum_test_loss, label='test')\n",
    "plt.title('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CbEeZ6R4EDYq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cifar10을 cnn으로 학습시키기.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
