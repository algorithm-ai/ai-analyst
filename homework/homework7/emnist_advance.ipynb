{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "emnist_advance.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks4BKH9BRe4_",
        "colab_type": "text"
      },
      "source": [
        "# 학습 목적 \n",
        "\n",
        "이전에 학습 했던 것을 기반으로 좀 더 모델을 발전시켜 봅니다.<br>\n",
        "해당 데이터셋에 맞는 모델 및 hyper parameter을 찾아봅니다. \n",
        "\n",
        "---------\n",
        "\n",
        "# 실행 순서 \n",
        "------\n",
        "+ ### 1. DataDownload \n",
        "+ ### 2. EDA \n",
        " #### 1. train, test pixel 값의 EDA\n",
        " #### 2. train ,test class 별 데이터 갯수 \n",
        "\n",
        "+ ### 3. DNN 구성.\n",
        " - **layer 별 학습**\n",
        " - **Weight 갯수 별 학습**\n",
        " - **Regularization 적용**\n",
        "    - Dropout \n",
        "    - L2, L1 Regularization\n",
        " - **batch 별 학습**\n",
        " - **Optimizer**\n",
        "    - adam optimizer \n",
        "    - momentum with nesterov\n",
        "    - rmsp 으로 모델 학습\n",
        "\n",
        "+ ### 4. metric 측정 \n",
        " - (tensorboard accuracy, loss step별로 측정)\n",
        "\n",
        "+ ### 5. analysis \n",
        " - 각 층별 weights, bias 평균의 변화 측정 (step 별)*이탤릭체 텍스트*\n",
        "------\n",
        "\n",
        "\n",
        "# 실행방법: \n",
        "\n",
        "- layer , weight ,Regularization, batch을 바꿔가며 10000 step 정도 수행해 봅니다.<br>\n",
        "그 후 가장 최적의 hyper parameter 을 찾은 후 optimizer 을 바꿔가며 최적화 해 보세요!\n",
        "\n",
        "+ ###  HINT 1\n",
        "\n",
        " + #### layer의 깊이(deep)와 넓이(wide)을 바꾸는 방법\n",
        " \n",
        "    ```python\n",
        "    # layer의 깊이(deep) 설정하기 \n",
        "num_layers = 5\n",
        "layer_units = tf.placeholder(dtype=tf.int64 ,shape=[num_layers], name='layer_units') \n",
        "\n",
        "    # 위에서 layer의 깊이를 설정했다면 아래 Hyperparameter 설정(왼쪽 화살표의 목차를 확인하세요) 에서 units 를 변경해주세요  \n",
        "    # ❖(주의!) num_outputs은 마지막에 logits 출력 갯수이기 때문에 변경하면 안됩니다. \n",
        "\n",
        "    # 첫번째 층의 unit 갯수 : 16\n",
        "    # 두번째 층의 unit 갯수 : 32\n",
        "    # ... \n",
        "    # logit층의 unit 갯수 : 26  \n",
        "    units = [16, 32, 64, 128, num_outputs]\n",
        "    ```\n",
        "\n",
        "\n",
        "+ ### HINT 2\n",
        "\n",
        "    + #### Regularization 적용 방법 \n",
        "     + L1, L2 Regularization 을 적용한 코드는 **LOSS**(왼쪽 상단 화살표의 목차를 확인하세요) 에 있습니다. <br>\n",
        "해당 부분에서 선생님들이 변경하는 건 **lambda** 부분입니다. <br>\n",
        "placeholder로 **lambda** 을 만들었으며 l1, l2 따로 만들었습니다. <br>**lambda** 을 0.1~0.0001 까지 바꿔보면서 해당 데이터셋의 최적의 l2, l1 lambda 을 찾아보세요.\n",
        "(일반적으로 l1, l2 lambda을 동시에 사용하지 않습니다.)\n",
        "\n",
        "\n",
        "+ ### HINT 3\n",
        "    + #### Dropout 적용 방법 \n",
        "\n",
        "        + BUILD_MODEL 부분에 구현 되어 있으며 구현 코드는 아래와 같습니다\n",
        "        ```python\n",
        "                            ...\n",
        "        if not layer_idx == n_layers-1:\n",
        "\n",
        "            # Dropout 은 relu에는 통상적으로 non-linear function 전에 적용합니다. \n",
        "            layer = tf.cond(phase_train,\n",
        "                            lambda: tf.nn.dropout(layer, rate=drop_rate),\n",
        "                            lambda: tf.nn.dropout(layer, rate=0.0))\n",
        "                            ....\n",
        "\n",
        "        ```\n",
        "\n",
        "    placeholder 로 dropout 을 적용할 비율을 입력받습니다. 해당 변수는 Input_Layer(목차 참조)에서 찾아보세요. <br>\n",
        "    변수를 찾은 후 training , feed_dict 에서 unit 을 drop 할 비율을 변경하면서 최적의 drop 비율을 찾아보세요. \n",
        "\n",
        "\n",
        "+ ### HINT 4\n",
        "    + #### Batch size 적용 방법 \n",
        "        Hyperparameter(목차 참조)에서 해당 변수를 찾아 변경하세요. \n",
        "\n",
        "+ ### HINT 5\n",
        "    + #### optimizer 적용 방법 \n",
        "\n",
        "        optimizer 는 아래 코드를 참조하세요.\n",
        "        ```python\n",
        "        tf.train.AdamOptimizer(lr).minimize(loss)\n",
        "        tf.train.RMSPropOptimizer(lr).minimize(loss)\n",
        "        tf.train.MomentumOptimizer(lr).minimize(loss)\n",
        "        ```\n",
        "\n",
        "+ ### HINT 6\n",
        "    + #### learning rate 적용 방법 \n",
        "        Hyperparameter(목차 참조)에서 해당 변수를 찾아 변경하세요. \n",
        "\n",
        "        ---------------\n",
        "\n",
        "        ```python\n",
        "        accum_train_acc = [] \n",
        "        accum_train_loss = [] \n",
        "        accum_test_acc = [] \n",
        "        accum_test_loss = [] \n",
        "        ```\n",
        "각 list 에 train_acc, train_loss, test_acc, test_loss 가 들어가있습니다. 해당 리스트를 test 시 max accuracy , min loss 을 찾아주세요! \n",
        "\n",
        "# 제출 포맷 및 예\n",
        "\n",
        "\n",
        "<style type=\"text/css\">\n",
        ".tg  {border-collapse:collapse;border-spacing:0;}\n",
        ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
        ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
        ".tg .tg-0lax{text-align:left;vertical-align:top}\n",
        ".tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}\n",
        "</style>\n",
        "<table class=\"tg\">\n",
        "  <tr>\n",
        "    <th class=\"tg-0lax\">Name</th>\n",
        "    <th class=\"tg-0lax\">Values</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-0lax\">layer units </td>\n",
        "    <td class=\"tg-0lax\">16,32,64,128, 26</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-0lax\">batch size</td>\n",
        "    <td class=\"tg-0lax\">120</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-0lax\">learning rate</td>\n",
        "    <td class=\"tg-0lax\">0.1</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-0pky\">max step</td>\n",
        "    <td class=\"tg-0lax\">50000</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-0pky\">dropout rate</td>\n",
        "    <td class=\"tg-0lax\">0.2</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-0pky\">l1, l2 regularization </td>\n",
        "    <td class=\"tg-0lax\">l2</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-0lax\">l1, l2 regularization lambda </td>\n",
        "    <td class=\"tg-0lax\">0.03</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-0lax\">max accuracy </td>\n",
        "    <td class=\"tg-0lax\">87.35</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-0lax\">min loss </td>\n",
        "    <td class=\"tg-0lax\">0.00017</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-0lax\">optimizer </td>\n",
        "    <td class=\"tg-0lax\">Momentum with Nesterov , momentum 0.999</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZxGJKYeeQ9_",
        "colab_type": "text"
      },
      "source": [
        "### 1. EMNIST DataDownLOAD "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMrmssZfQzxh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install emnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb_xpB9MQ5-q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import emnist\n",
        "import matplotlib.pyplot as plt \n",
        "import numpy as np "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_guXh6WYQ_Gs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_images, train_labels = emnist.extract_training_samples('letters')\n",
        "test_images, test_labels = emnist.extract_test_samples('letters')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQ-hEcJURVJK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# show sample data \n",
        "print(train_images.shape)\n",
        "plt.imshow(train_images[0])\n",
        "plt.show()\n",
        "plt.imshow(train_images[1])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TgDylTThGF4",
        "colab_type": "text"
      },
      "source": [
        "#### Image EDA "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4rXGPknhSwr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train Image \n",
        "n_train = len(train_images)\n",
        "seq_length = train_images.shape[1:3]\n",
        "\n",
        "train_min = train_images.reshape([-1,784]).min(axis=1)\n",
        "train_max = train_images.reshape([-1,784]).max(axis=1)\n",
        "\n",
        "print('# train : {}'.format(n_train))\n",
        "print('seq_length : {}'.format(seq_length))\n",
        "\n",
        "# 아래 그림으로 보아 모든 Test 이미지가 최소값은 0, 최대값은 255을 가지고 있음. \n",
        "plt.scatter(train_min, train_max)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxJIXlDGkWkG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test Image \n",
        "n_test = len(test_images)\n",
        "seq_length = test_images.shape[1:3]\n",
        "\n",
        "test_min = test_images.reshape([-1,784]).min(axis=1)\n",
        "test_max = test_images.reshape([-1,784]).max(axis=1)\n",
        "\n",
        "print('# train : {}'.format(n_test))\n",
        "print('seq_length : {}'.format(seq_length))\n",
        "\n",
        "# 아래 그림으로 보아 모든 Test 이미지가 최소값은 0, 최대값은 255을 가지고 있음. \n",
        "plt.scatter(test_min, test_max)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hox-JI_akSfw",
        "colab_type": "text"
      },
      "source": [
        "#### Label EDA "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K47CeLpSjBsB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('train label data shape : {}'.format(train_labels.shape))\n",
        "print('test label data shape : {}'.format(test_labels.shape))\n",
        "\n",
        "print(np.unique(train_labels, return_counts=True))\n",
        "print(np.unique(test_labels, return_counts=True))\n",
        "\n",
        "train_labels = train_labels - 1 \n",
        "test_labels = test_labels - 1 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqpZfH91hTWf",
        "colab_type": "text"
      },
      "source": [
        "### Normalize and Normalization checking  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qUP6B8KS3t0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.unique([train_labels],return_counts=True)\n",
        "\n",
        "# Normalization\n",
        "train_images = train_images.reshape(-1,784)/ 255.\n",
        "test_images = test_images.reshape(-1,784) / 255.\n",
        "\n",
        "# Check Normalization \n",
        "def chk_norm(input_data, min_value=0, max_value=1):\n",
        "    \"\"\"\n",
        "    input_data : Ndarray, \n",
        "    min_value : int, input_data의 모든 element는 min_value 이상의 값을 가져야함 \n",
        "    max_value : int, input_data의 모든 element는 max_value 이하의 값을 가져야함 \n",
        "\n",
        "    description: input_data의 모든 element가 특정 범위(min_value ~ max_value)에 있는지 확인합니다.\n",
        "    \"\"\"\n",
        "    input_data = np.asarray(input_data)\n",
        "    assert np.all(input_data <= max_value) & np.all(input_data >= min_value), 'normalize가 잘못 되었습니다.'\n",
        "\n",
        "\n",
        "chk_norm(train_images)    \n",
        "chk_norm(test_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EILqz9ejk3tf",
        "colab_type": "text"
      },
      "source": [
        "## Input layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h66szIRgcCa9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf \n",
        "num_inputs = 28*28 # MNIST Input size\n",
        "num_outputs = 26 # The number of Label : 26\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Data를 받아오는 placeholder\n",
        "x = tf.placeholder(tf.float32, shape=[None, num_inputs],name='x')\n",
        "\n",
        "#labels shape : [1,3,7,7,4,..]\n",
        "labels_cls = tf.placeholder(tf.int32, shape=[None], name='labels') \n",
        "\n",
        "# scalar 을 onehot-vector 형태로 변환합니다.\n",
        "labels = tf.one_hot(labels_cls, depth=num_outputs)\n",
        "\n",
        "# phase_train \n",
        "phase_train = tf.placeholder(tf.bool, shape=[], name='phase_train')\n",
        "\n",
        "# Dropout 시 units이 0으로 될 확률.\n",
        "drop_rate = tf.placeholder(tf.float32, shape=[], name='drop_rate')\n",
        "\n",
        "# Hparam\n",
        "learning_rate = tf.placeholder_with_default(0.1,shape=(), name='learning_rate')\n",
        "\n",
        "# TODO: shape 을 고정하지 않고 하는 방법도 생각해 보세요\n",
        "num_layers=5\n",
        "layer_units = tf.placeholder(dtype=tf.int64 ,shape=[num_layers], \n",
        "                             name='layer_units') \n",
        "l2_lambda = tf.placeholder_with_default(0.0, shape=[], name='l2_lambda') \n",
        "l1_lambda = tf.placeholder_with_default(0.0, shape=[], name='l1_lambda')\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDJ5cNp7pSrU",
        "colab_type": "text"
      },
      "source": [
        "# BUILD MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAmNm5S0Epwr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DNN Archithecture\n",
        "\n",
        "# layer 길이 \n",
        "n_layers = int(layer_units.get_shape()[0])\n",
        "\n",
        "# layer 구축\n",
        "in_ch = num_inputs\n",
        "layer = x \n",
        "for layer_idx in range(n_layers):\n",
        "    with tf.variable_scope('layer_{}'.format(layer_idx)):\n",
        "                \n",
        "        w = tf.Variable(tf.random.normal([in_ch, layer_units[layer_idx]],  \n",
        "                                         stddev=np.sqrt(2/num_inputs)),\n",
        "                        name='weight', validate_shape=False)\n",
        "        tf.add_to_collection('weights', w)\n",
        "        \n",
        "        b = tf.Variable(tf.zeros([layer_units[layer_idx]])\n",
        "                        ,name='bias',validate_shape=False)\n",
        "        layer = tf.matmul(layer, w) + b\n",
        "        \n",
        "        # logits 에는 Activation 을 적용 하지 않습니다.\n",
        "        if not layer_idx == n_layers-1:\n",
        "\n",
        "            # Dropout 은 relu에는 통상적으로 non-linear function 전에 적용합니다. \n",
        "            layer = tf.cond(phase_train,\n",
        "                            lambda: tf.nn.dropout(layer, rate=drop_rate),\n",
        "                            lambda: tf.nn.dropout(layer, rate=0.0))\n",
        "            layer = tf.nn.relu(layer)\n",
        "            in_ch = layer_units[layer_idx]\n",
        "            layer = layer\n",
        "\n",
        "logits = tf.identity(layer,'logits')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSr4538goC7W",
        "colab_type": "text"
      },
      "source": [
        "## LOSS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CyhshyNl4eM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# weights 만 가져옵니다. \n",
        "weights = tf.get_collection('weights')\n",
        "\n",
        "# CEE loss\n",
        "loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels)\n",
        "\n",
        "# L1 Regularization : 0.5 * lambda *  ∑(weight)\n",
        "l1_loss = tf.add_n([tf.reduce_sum(weight) for weight in weights])*l1_lambda*0.5\n",
        "\n",
        "# L2 Regularization : 0.5 * lambda * ∑(weight**2)\n",
        "l2_loss = tf.add_n([tf.reduce_sum(weight**2) for weight in weights])*l2_lambda*0.5\n",
        "\n",
        "# CEE + L1 or L2 Regularization\n",
        "total_loss = tf.reduce_mean(loss) + l1_loss + l2_loss "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45BsTCbOmg1l",
        "colab_type": "text"
      },
      "source": [
        "## Train op"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBvlXF1KmkN5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## high level api을 이용해 Gradient Descent 구현\n",
        "train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6GYOBE4oETP",
        "colab_type": "text"
      },
      "source": [
        "## next batch function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xISpx1Stner-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 복원 추출 합니다. \n",
        "import random\n",
        "def next_batch(xs, ys, batch_size):\n",
        "    indices = random.sample(range(len(ys)), batch_size)\n",
        "    return xs[indices], ys[indices]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijhDLH38pU0G",
        "colab_type": "text"
      },
      "source": [
        "## Metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZBTnlf1slyP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logits_cls = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
        "acc = tf.reduce_mean(tf.cast(tf.equal(logits_cls, labels_cls), \n",
        "                     dtype=tf.float32))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6_kPJXzVSRz",
        "colab_type": "text"
      },
      "source": [
        "## Hyperparam 설정\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0j6oBKBVY_a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# layer 길이 및 unit 갯수 \n",
        "# placeholder에서 unit 의 갯수를 5개로 고정 시켰습니다. \n",
        "units = [16, 32, 64, 128, num_outputs]\n",
        "\n",
        "# batch size \n",
        "batch_size = 120 \n",
        "\n",
        "#l2 loss decay\n",
        "l2_decay = 0.0\n",
        "\n",
        "#l1 loss decay\n",
        "l1_decay = 0.0\n",
        "\n",
        "#drop rate \n",
        "# 만약 0.1 이라면 0.1 의 확률로 unit 의 값을 0으로 만듭니다. \n",
        "drop_prob = 0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jB6ptx6ooW1c",
        "colab_type": "text"
      },
      "source": [
        "## Session Open"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rf2miWhBoQx4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess = tf.Session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRQxo6aNnrtX",
        "colab_type": "text"
      },
      "source": [
        "## Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-gFoZ5IoIq4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess.run(tf.global_variables_initializer(), feed_dict={layer_units:units})\n",
        "max_iter = 20000 \n",
        "mean_gradients=[]\n",
        "accum_train_acc = [] \n",
        "accum_train_loss = [] \n",
        "accum_test_acc = [] \n",
        "accum_test_loss = [] \n",
        "\n",
        "for i in range(max_iter):\n",
        "    # random select input data, labels\n",
        "    batch_xs, batch_ys = next_batch(train_images, train_labels, batch_size)\n",
        "\n",
        "    # Training \n",
        "    train_acc, train_loss, _ = sess.run([acc, total_loss, train_op], \n",
        "                                        feed_dict={x: batch_xs,\n",
        "                                                   labels_cls: batch_ys,\n",
        "                                                   learning_rate:0.1,                                                   \n",
        "                                                   l2_lambda:l2_decay,\n",
        "                                                   l1_lambda:l1_decay,\n",
        "                                                   drop_rate:drop_prob,\n",
        "                                                   phase_train:True,\n",
        "                                                  })\n",
        "    # Evaluating\n",
        "    if i % 100 == 0:\n",
        "        test_acc, test_loss = sess.run([acc, total_loss], \n",
        "                                    feed_dict={x:test_images,\n",
        "                                               labels_cls: test_labels,\n",
        "                                               phase_train: False,\n",
        "                                               drop_rate:1.0,                                               \n",
        "                                              })\n",
        "        print('step : {} train_acc : {:.4f} train_loss : {:.4f} validation acc : {:.4f} validation loss : {:.4f}'.\\\n",
        "              format(i, train_acc, train_loss, test_acc, test_loss))\n",
        "        \n",
        "        # list 에 추출함.\n",
        "        accum_train_acc.append(train_acc)\n",
        "        accum_train_loss.append(train_loss)\n",
        "        accum_test_acc.append(test_acc)        \n",
        "        accum_test_loss.append(test_loss)        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3h8WbvEpx8Y",
        "colab_type": "text"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGoYUbMLyiQC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(range(0,20000,100), accum_train_acc, label='train')\n",
        "plt.plot(range(0,20000,100), accum_test_acc, label='test')\n",
        "plt.title('Accuracy')\n",
        "plt.legend()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUkp2Nlv0odN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(range(0,20000,100), accum_train_loss, label='train')\n",
        "plt.plot(range(0,20000,100), accum_test_loss, label='test')\n",
        "plt.title('Loss')\n",
        "plt.legend()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}