# -*- coding: utf-8 -*-
"""conv_config.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p2ZwButVgOGNHIHEUIffxT08_XxbAiPZ
"""

class Config(object):
    def __init__(self, **hparam):
        """
        Description: 
            1. Convolution, Pooling 의 strides 는 늘 2로 고정되어 있습니다. 
            2. Convolution, Pooling 의 padding 은 늘 'SAME' 으로 고정되어 있습니다. 
            
            hparame 에는 아래와 같은 파라미터가 반드시 들어가야 합니다. 
            lr : optimizer 에 적용될 학습률 입니다. 
            
            batch_size : 한 step 에 사용할 data 갯수 입니다. 

            l2_lambda : l2 정규화를 얼만큼 적용할 것인지 결정합니다. 
                해당 값이 클수록 l2 정규화 효과가 커집니다.  
                lambda 값은 1보다 작고 0보다 크거나 같아야 합니다. 
                lambda 값이 0이면 정규화 효과가 없습니다. 
            l1_lambda : l1 regularization을 얼만큼 적용할 것인지 결정합니다. 
                해당 값이 클수록 l1 정규화 효과가 커집니다.  
                lambda 값은 1보다 작고 0보다 크거나 같아야 합니다. 
                lambda 값이 0이면 정규화 효과가 없습니다. 
            drop_rate  : unit 을 Drop 할 확률입니다.
                만약 drop_rate=0.5 이면 unit 은 50% 확률로 0이 됩니다. 
            optimizer :
                optimizer 는 총 4가지 종류가 준비되어 있습니다. 
                'adam', 'gd', 'momentum', 'rmps' 
            kernel_sizes : CNN 에 적용할 kernel size 입니다. 아래와 같은 예시로 입력되야 합니다.
                ex) [(5,5), (3,3), (3,3), (2,2)] 

            output_channels : CNN에 적용할 units 갯수 입니다. 아래와 같은 예시로 입력 되어야 합니다. 

            pooling_schedule : activation 후 pooling 을 적용시킬지 말지 결정합니다. 
                pooling 에 적용되는 strides와 kernel_size 는 고정되어 있습니다.
                strides 가 [1, 2, 2, 1],  kerne_size 는 [1, 2, 2, 1]로 고정 되어 있습니다. 
                
                사용방법은 아래와 같습니다. 
                ex) [False, False, True, False]
                
                첫번째 두번째, 네번째 레이어는 max pooling 을 적용하지 않고 
                3번째 레이어만 pooling 을 적용합니다. 
        """

        self.lr = hparam['lr']
        self.batch_size = hparam['batch_size']
        self.l2_lambda = hparam['l2_lambda']
        self.l1_lambda = hparam['l1_lambda']
        self.drop_rate = hparam['drop_rate']
        self.optimizer = hparam['optimizer']
        self.conv_kernels = hparam['conv_kernels']
        self.conv_units = hparam['conv_units']
        self.pooling_schedule = hparam['pooling_schedule']
        self.fc_units = hparam['fc_units'] 
        self.max_iter = hparam['max_iter'] 
        self.ckpt = hparam['ckpt']

config_0 = Config(lr=0.01, batch_size=120, l2_lambda=0.01,
                l1_lambda=0.0001, drop_rate=0.5, optimizer='adam',
                conv_kernels=[(5,5), (3,3), (3,3), (3, 3)],
                conv_units=[32, 64, 128, 256],
                pooling_schedule=[True, True, True, True],
                fc_units=[256,256,26],
                max_iter=20000,
                ckpt=100)

config_dict = {'0': config_0}