{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ayJmuIxw-EYT"
   },
   "source": [
    "╔══<i><b>Alai-DeepLearning</b></i>════════════════════════════╗\n",
    "###  &nbsp;&nbsp; **✎&nbsp;&nbsp;Week 13. RNN-Basis**\n",
    "# Section 3. RNN에서의 Feed Forward\n",
    "\n",
    "\n",
    "### _Objective_\n",
    "\n",
    "1. 순환신경망(RNN)에서 어떤 순서로 동작하는 지 알아보도록 하겠습니다. <br>\n",
    "2. RNN은 Tensorflow Low-API로 작성하는 것보다, Keras High-API를 위주로 작성하겠습니다.\n",
    "\n",
    "╚═════════════════════════════════════════╝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "CUqMZl4n-EYV",
    "outputId": "ca001262-8947-4c7f-92e7-638b1de9cf9e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eofvrZRd-EYZ"
   },
   "source": [
    "<br><br>\n",
    "\n",
    "# \\[ 1. RNN의 Feed Forward  \\]\n",
    "---\n",
    "---\n",
    "\n",
    "> *RNN은 시간적 순서(time step)으로 처리되는 연산집합 `Cell`을 순환하며 처리합니다.* <br>\n",
    "> *Hello라는 Sequence를 출력하는 RNN 모델을 학습해보도록 하겠습니다.*<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yoXW0SSH-EYZ"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 1. 철자를 Vector로 표현하기\n",
    "---\n",
    "\n",
    "* 우리는 철자를 숫자로 표현하기 위해, 가장 간단한 방법 중 하나인 one-hot Vector를 이용하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t7wr1sjl-EYa"
   },
   "source": [
    "### (1) One hot Vector로 표현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vrHPrrYl-EYb"
   },
   "source": [
    "| Char | index |One-Hot Vector|\n",
    "|--- |---|---| \n",
    "| h | 0 | [1,0,0,0,0] |\n",
    "| e | 1 | [0,1,0,0,0] |\n",
    "| l | 2 | [0,0,1,0,0] |\n",
    "| o | 3 | [0,0,0,1,0] |\n",
    "| <EOS\\> | 4 | [0,0,0,0,1] |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AWG7m8Y_-EYb"
   },
   "outputs": [],
   "source": [
    "char2vec = {'h':np.array([1,0,0,0,0]),\n",
    "            \"e\":np.array([0,1,0,0,0]),\n",
    "            \"l\":np.array([0,0,1,0,0]),\n",
    "            'o':np.array([0,0,0,1,0]),\n",
    "            '<EOS>':np.array([0,0,0,0,1])}\n",
    "\n",
    "idx2char = ['h','e','l','o','<EOS>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "U3Mjhuwx-EYd",
    "outputId": "78563afd-d28f-452d-b189-411ead7f1189"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character -> Vector : 'l' ->  [0 0 1 0 0]\n",
      "index -> Character : '2' ->  l\n"
     ]
    }
   ],
   "source": [
    "print(\"Character -> Vector : 'l' -> \", char2vec['l'])\n",
    "print(\"index -> Character : '2' -> \", idx2char[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RGRi0R6B-EYf"
   },
   "source": [
    "### (2) Embedding Vector로 표현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GDRZtXVZ-EYf"
   },
   "source": [
    "훨씬 많은 단어 혹은 철자를 처리할 때에는 One-hot Vector보다는 `embedding` Layer를 많이 이용합니다.<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qh3Gzv02-EYg"
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Embedding, Input\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "p2h7D6MW-EYh",
    "outputId": "cb5eba74-742f-4d74-cac4-43de18bc06ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "tf.set_random_seed(1)\n",
    "\n",
    "inputs = Input(shape=())\n",
    "embeded = Embedding(input_dim=5,output_dim=2)(inputs)\n",
    "\n",
    "embeded\n",
    "\n",
    "model = Model(inputs,embeded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sL7_vL8S-EYj"
   },
   "source": [
    "`embedding` Layer는 난수로 채워진 Matrix을 만들어 줍니다. 우리는 `embedding` layer에서 받은 Input은 Matrix의 각 행에 대한 인덱스로 행의 값들을 반환하게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gMVV_xIj-EYk",
    "outputId": "f47a7ef5-ac83-4029-b358-83bfa32cc1c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00211157,  0.04429672],\n",
       "       [-0.01521801, -0.02037966],\n",
       "       [-0.02079648, -0.02168524],\n",
       "       [-0.0420602 ,  0.04103562],\n",
       "       [-0.02007725,  0.02599252]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding Layer 내 Weight 가져오기\n",
    "sess = K.get_session()\n",
    "graph = sess.graph\n",
    "embedding_weight = graph.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)[0]\n",
    "\n",
    "sess.run(embedding_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J_tk68AS-EYm"
   },
   "source": [
    "|char|index|1번째 임베딩 벡터 값|2번째 임베딩 벡터 값|\n",
    "|----|----|----|----|\n",
    "|\"h\"| 0| 0.002 | 0.044 |\n",
    "|\"e\"| 1| -0.015 | -0.020 |\n",
    "|\"l\"| 2| -0.021 | -0.022 |\n",
    "|\"o\"| 3| -0.042 | 0.041 |\n",
    "|\"<eos\\>\"| 4| -0.020 | 0.026 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OGMW3HNE-EYm",
    "outputId": "69e16793-8280-4a3f-8161-fb9580f59121"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 0 : [[0.00211157 0.04429672]]\n",
      "index 1 : [[-0.01521801 -0.02037966]]\n",
      "index 2 : [[-0.02079648 -0.02168524]]\n"
     ]
    }
   ],
   "source": [
    "print(\"index 0 : {}\".format(model.predict([[0]])))\n",
    "print(\"index 1 : {}\".format(model.predict([[1]])))\n",
    "print(\"index 2 : {}\".format(model.predict([[2]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mWjUYpUO-EYo"
   },
   "source": [
    "예를 들어 한국어로 표현할 수 있는 음절 수는 총 11,172자로 이를 바로 one-hot vector로 표시하면 11,172차원의 공간으로 Mapping되기 때문에, 지나치게 Sparse하게 표현됩니다. <br>\n",
    "이런 경우 Embedding Layer은 훨씬 작은 차원으로 맵핑시킬 수 있습니다. 위의 Embedding Layer의 Mapping은 Weight로 구성되어 있기 때문에, 처음에는 무작위로 배치되어 있다가, 학습해 감에 따라,각각의 철자의 Embedding Vector가 바뀝니다.\n",
    "\n",
    "\n",
    "reference : \n",
    "1. [How to use word embedding layers for deep learning with keras](https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hDSrRA6z-EYp"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 2. Time Step 별 계산\n",
    "\n",
    "----\n",
    "\n",
    "* 보다 직관적인 계산을 위해 철자는 one-hot vector로 구성하여 계산하도록 하겠습니다.<br>\n",
    "* RNN은 Time Step 별로 가중치를 공유합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1G5qDSnM-EYp"
   },
   "source": [
    "### (1) 가중치 구성하기\n",
    "\n",
    "기학습한 가중치를 이용하도록 하겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B3ZEnyiI-EYq"
   },
   "outputs": [],
   "source": [
    "### RNN Cell 내 가중치\n",
    "w_xh = np.array([[-2.6, -1.6, -2.1],\n",
    "                 [ 1.2,  0.4,  0.3],\n",
    "                 [ 2.1,  1.9, -0.7],\n",
    "                 [-1.4, -1.5,  2.5],\n",
    "                 [-0.9,  0.4, -0.9]])\n",
    "\n",
    "w_hh = np.array([[-0.5, -2.3,  2.9],\n",
    "                 [ 1.9,  1.5,  1.7],\n",
    "                 [-0.7, -1.2,  1.5]])\n",
    "\n",
    "b_h = np.array([-0.5, -0.4, -1. ])\n",
    "\n",
    "# Output Layer 내 가중치\n",
    "w_hy = np.array([[ 0.3, -2.6,  1.2,  2.6, -1.1],\n",
    "                 [-1.1, -2.4,  2.2,  1.6, -2.4],\n",
    "                 [-0.4, -3.1, -3. ,  3.6,  3. ]])\n",
    "\n",
    "b_y = np.array([-1.8, -0.5,  1.3,  0.1,  0.8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e0kbqk_5-EYr"
   },
   "source": [
    "### (2) Time Step 1에서의 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VbVmR6PI-EYs"
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WSSt_ZoP-EYt",
    "outputId": "c647a0cc-97df-4c8f-d707-a011478fc0c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1번째 Timestamp의 hidden : [-0.99594936 -0.96402758 -0.99594936]\n",
      "1번째 Timestamp의 output : [-0.63997473  7.49057754  0.97184817 -7.61733016  1.22136241]\n",
      "1번째 Timestamp의 result : e\n"
     ]
    }
   ],
   "source": [
    "init_h = np.array([0,0,0])\n",
    "x_0 = char2vec['h']\n",
    "\n",
    "a_1 = np.dot(init_h, w_hh) + np.dot(x_0,w_xh) + b_h\n",
    "h_1 = np.tanh(a_1)\n",
    "\n",
    "y_1 = np.dot(h_1, w_hy) + b_y\n",
    "o_1 = softmax(y_1)\n",
    "\n",
    "print(\"1번째 Timestamp의 hidden : {}\".format(h_1))\n",
    "print(\"1번째 Timestamp의 output : {}\".format(y_1))\n",
    "print(\"1번째 Timestamp의 result : {}\".format(idx2char[np.argmax(o_1)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QMApQoMh-EYv"
   },
   "source": [
    "### (2) Time Step 2에서의 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9JwRUeCf-EYw",
    "outputId": "c992bf18-ec88-4cd2-9c80-031a1fc21bb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2번째 Timestamp의 hidden : [ 0.06340167  0.96673299 -0.99999709]\n",
      "2번째 Timestamp의 output : [-2.44438695  0.11498748  6.50288586 -1.78837242 -4.58989229]\n",
      "2번째 Timestamp의 result : l\n"
     ]
    }
   ],
   "source": [
    "x_1 = char2vec['e']\n",
    "\n",
    "a_2 = np.dot(h_1, w_hh) + np.dot(x_1, w_xh) + b_h\n",
    "h_2 = np.tanh(a_2)\n",
    "\n",
    "y_2 = np.dot(h_2, w_hy) + b_y\n",
    "o_2 = softmax(y_2)\n",
    "\n",
    "\n",
    "print(\"2번째 Timestamp의 hidden : {}\".format(h_2))\n",
    "print(\"2번째 Timestamp의 output : {}\".format(y_2))\n",
    "print(\"2번째 Timestamp의 result : {}\".format(idx2char[np.argmax(o_2)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m4XH28l--EYy"
   },
   "source": [
    "### (3) Time Step 3에서의 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "txIbG9KV-EYy",
    "outputId": "dba55473-e1ff-440e-b929-0a03e3dad820"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3번째 Timestamp의 hidden : [ 0.9994564  0.999335  -0.8793026]\n",
      "3번째 Timestamp의 output : [-2.24771054 -2.7711526   7.33579249  1.1320333  -5.33571385]\n",
      "3번째 Timestamp의 result : l\n"
     ]
    }
   ],
   "source": [
    "x_2 = char2vec['l']\n",
    "\n",
    "a_3 = np.dot(h_2, w_hh) + np.dot(x_2,w_xh) + b_h\n",
    "h_3 = np.tanh(a_3)\n",
    "\n",
    "y_3 = np.dot(h_3,w_hy)+b_y\n",
    "o_3 = softmax(y_3)\n",
    "\n",
    "print(\"3번째 Timestamp의 hidden : {}\".format(h_3))\n",
    "print(\"3번째 Timestamp의 output : {}\".format(y_3))\n",
    "print(\"3번째 Timestamp의 result : {}\".format(idx2char[np.argmax(o_3)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "alNHjN3Z-EY0"
   },
   "source": [
    "### (4) Time Step 4에서의 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7LbZJVW1-EY1",
    "outputId": "ff6355c4-8065-44c5-e56c-4c621e5e8f33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4번째 Timestamp의 hidden : [0.99855062 0.9419888  0.91834213]\n",
      "4번째 Timestamp의 output : [-2.90395935 -8.20386532  1.81560973  7.50944534  0.19584758]\n",
      "4번째 Timestamp의 result : o\n"
     ]
    }
   ],
   "source": [
    "x_3 = char2vec['l']\n",
    "\n",
    "a_4 = np.dot(h_3, w_hh) + np.dot(x_3,w_xh) + b_h\n",
    "h_4 = np.tanh(a_4)\n",
    "\n",
    "y_4 = np.dot(h_4,w_hy)+b_y\n",
    "o_4 = softmax(y_4)\n",
    "\n",
    "print(\"4번째 Timestamp의 hidden : {}\".format(h_4))\n",
    "print(\"4번째 Timestamp의 output : {}\".format(y_4))\n",
    "print(\"4번째 Timestamp의 result : {}\".format(idx2char[np.argmax(o_4)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5F_uOHVl-EY3"
   },
   "source": [
    "<br><br>\n",
    "\n",
    "# \\[ 2. Keras에서 RNN 구성하기  \\]\n",
    "---\n",
    "---\n",
    "\n",
    "> *텐서플로우에서의 RNN 구현은 매우 어렵습니다. 시계열 데이터는 동적인 데이터인 것에 반해, Tensorflow의 Graph는 정적이기 때문입니다.<br>\n",
    "현재 텐서플로우 내의 RNN 구현체는 Keras 스타일로 대체되고 있습니다. <br>\n",
    "이에 따라 텐서플로우 내 Keras 메소드들을 이용하여 구현하도록 하겠습니다.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JNkrXRss-EY4"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 1. Keras의 순환 신경망 구조 : RNN와 RNN Cell\n",
    "---\n",
    "\n",
    "* Keras에서는 연산을 처리하는 Class인 `RNN Cell`과 그것을 Time Step 별로 반복시켜 주는 Wrapper Class `RNN`으로 나누어 구성되어 있습니다.<br>\n",
    "* 우리는 Cell에서의 인터페이스와 연산을 구성하면, `RNN`을 통해 간단히 Time Step 별로 반복시켜 줄 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AxIb_RoV-EY4"
   },
   "source": [
    "### (1) RNNCell 구성하기\n",
    "\n",
    "우리는 반복하는 구간인 RNNCell을 아래와 같이 구성할 수 있습니다. <br>\n",
    "Keras는 하나의 인터페이스로, 꼭 구현해야 하는 요소들이 존재합니다. RNNCell을 구현하기 위해서는 아래 요소들을 지켜주어야 합니다. 이를 지켜주지 않으면 Wrapper Class인 RNN에서 정상적으로 동작하지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DvU49TZ3-EY5"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class RNNCell(Layer):\n",
    "    \"\"\"\n",
    "    Keras에서 Cell을 구성하는 Layer\n",
    "    \n",
    "    1. __init__ : Cell에 필요한 hyper-parameter를 구성\n",
    "       상태 벡터의 크기를 의미하는 self.state_size는 필수적으로 정해주어야 함\n",
    "       마지막에 super([Layer],self).__init__(**kwargs)를 호출해야함\n",
    "\n",
    "    2. build : Cell에서 관리하는 Weight들을 선언. \n",
    "       마지막에 super([Layer],self).build()를 호출해야함\n",
    "    \n",
    "    3. call : Cell의 연산 순서를 정의\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, n_units, **kwargs):\n",
    "        self.n_units = n_units\n",
    "        self.state_size = n_units\n",
    "        super(RNNCell, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.w_xh = self.add_weight(name='weight_xh',\n",
    "                                    shape=(input_shape[-1], self.n_units),\n",
    "                                    initializer=tf.initializers.glorot_normal())\n",
    "        self.w_hh = self.add_weight(name='weight_hh',\n",
    "                                    shape=(self.n_units, self.n_units),\n",
    "                                    initializer=tf.initializers.orthogonal())\n",
    "        self.b_h = self.add_weight(name='bias_h',\n",
    "                                   shape=(self.n_units,),\n",
    "                                   initializer=tf.initializers.zeros())\n",
    "        super(RNNCell, self).build(input_shape)\n",
    "        \n",
    "    def call(self, inputs, states):\n",
    "        prev_states = states[0]\n",
    "        h = (tf.matmul(inputs, self.w_xh) \n",
    "             + tf.matmul(prev_states, self.w_hh)\n",
    "             + self.b_h)\n",
    "        a = tf.tanh(h)\n",
    "        return a, [a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o7gvHeLg-EY7"
   },
   "source": [
    "### (2) Keras로 RNN 모델 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ownWL9iE-EY8"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, RNN\n",
    "\n",
    "n_inputs = 5 # Input 차원 수\n",
    "n_steps = 5 # time step의 크기\n",
    "n_neurons = 3 # Hidden 차원 수 \n",
    "n_outputs = n_inputs # Output 차원 수 \n",
    "\n",
    "inputs = Input(shape=(n_steps,n_inputs))\n",
    "hidden = RNN(RNNCell(n_neurons), return_sequences=True)(inputs)\n",
    "output = Dense(n_outputs, activation='softmax')(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eCI7bt6K-EY_"
   },
   "source": [
    "`RNN` Wrapper Class에서 반환할 수 있는 형태는 크게 2가지입니다.<br>\n",
    "\n",
    "* return_sequences : 각 time Step 별 Hidden State의 값을 반환\n",
    "* return_state : 마지막 time Step의 값을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CysKT_Cx-EZA",
    "outputId": "56d49997-c48c-4038-ff4f-be69ff24bd64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "return_Sequences=False 일때의 출력 값 형태:  (?, 3)\n",
      "return_Sequences=True 일때의 출력 값 형태:  (?, 5, 3)\n",
      "return_Sequences=True&return_state=True 일때의 첫번째 출력 값 형태:  (?, 5, 3)\n",
      "return_Sequences=True&return_state=True 일때의 두번째 출력 값 형태:  (?, 3)\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(n_steps,n_inputs))\n",
    "hidden = RNN(RNNCell(n_neurons), return_sequences=False)(inputs)\n",
    "print(\"return_Sequences=False 일때의 출력 값 형태: \",hidden.shape)\n",
    "\n",
    "hidden = RNN(RNNCell(n_neurons), return_sequences=True)(inputs)\n",
    "print(\"return_Sequences=True 일때의 출력 값 형태: \",hidden.shape)\n",
    "\n",
    "hidden = RNN(RNNCell(n_neurons), return_sequences=True, \n",
    "             return_state=True)(inputs)\n",
    "print(\"return_Sequences=True&return_state=True 일때의 첫번째 출력 값 형태: \",hidden[0].shape)\n",
    "print(\"return_Sequences=True&return_state=True 일때의 두번째 출력 값 형태: \",hidden[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wv_lNF-2-EZC"
   },
   "source": [
    "### (3) Keras를 통해 출력값 계산하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eB_WrBuF-EZD"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "n_inputs = 5 # Input 차원 수\n",
    "n_steps = 5 # time step의 크기\n",
    "n_neurons = 3 # Hidden 차원 수 \n",
    "n_outputs = n_inputs # Output 차원 수 \n",
    "\n",
    "inputs = Input(shape=(n_steps,n_inputs))\n",
    "hidden = RNN(RNNCell(n_neurons), return_sequences=True)(inputs)\n",
    "output = Dense(n_outputs, activation='softmax')(hidden)\n",
    "\n",
    "model = Model(inputs, output)\n",
    "\n",
    "# pretrained weight들로 setting\n",
    "model.set_weights([w_xh, w_hh,b_h,w_hy,b_y]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b6BuW0wt-EZE",
    "outputId": "933a21ea-edc3-4b5c-9326-aaaaeb1c634c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력값 :  ['h', 'e', 'l', 'l', 'o']\n",
      "출력값 :  ['e', 'l', 'l', 'o', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "input_values = \"hello\"\n",
    "print(\"입력값 : \",list(input_values))\n",
    "input_vecs = np.stack([char2vec[char] \n",
    "                       for char in input_values])[np.newaxis]\n",
    "\n",
    "results = model.predict(input_vecs)\n",
    "result_indices = np.argmax(results,axis=-1)[0]\n",
    "print(\"출력값 : \",[idx2char[idx] for idx in result_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LNK4KoB1-EZG"
   },
   "source": [
    "### (4) 기존의 Keras API를 활용하기\n",
    "\n",
    "위와 같이 우리가 직접구현한 RNNCell은 `SimpleRNNCell`로 이미 구현되어 있습니다.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dD60wgDj-EZH"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import SimpleRNNCell\n",
    "\n",
    "inputs = Input(shape=(n_steps,n_inputs))\n",
    "hidden = RNN(SimpleRNNCell(n_neurons), return_sequences=True)(inputs)\n",
    "output = Dense(n_outputs, activation='softmax')(hidden)\n",
    "\n",
    "model = Model(inputs, output)\n",
    "\n",
    "# pretrained weight들로 setting\n",
    "model.set_weights([w_xh, w_hh,b_h,w_hy,b_y]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IwXQ0yrr-EZI",
    "outputId": "436a10ff-fad0-47e7-e826-04e4e64e1125"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력값 :  ['h', 'e', 'l', 'l', 'o']\n",
      "출력값 :  ['e', 'l', 'l', 'o', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "input_values = \"hello\"\n",
    "print(\"입력값 : \",list(input_values))\n",
    "input_vecs = np.stack([char2vec[char] \n",
    "                       for char in input_values])[np.newaxis]\n",
    "\n",
    "results = model.predict(input_vecs)\n",
    "result_indices = np.argmax(results,axis=-1)[0]\n",
    "print(\"출력값 : \",[idx2char[idx] for idx in result_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tRnaTy9I-EZK"
   },
   "source": [
    "그리고 Cell에 RNN까지 Wrapping해놓은 것은 `SimpleRNN`이라는 이름으로 제공되고 있습니다.<br>\n",
    "`SimpleRNN`을 이용하면 보다 간결하게 작성할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x-BohiIB-EZK"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import SimpleRNN\n",
    "\n",
    "inputs = Input(shape=(n_steps,n_inputs))\n",
    "hidden = SimpleRNN(n_neurons, return_sequences=True)(inputs)\n",
    "output = Dense(n_outputs, activation='softmax')(hidden)\n",
    "\n",
    "model = Model(inputs, output)\n",
    "\n",
    "# pretrained weight들로 setting\n",
    "model.set_weights([w_xh, w_hh,b_h,w_hy,b_y]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vbT6bwFa-EZM",
    "outputId": "a084815b-b7e3-451c-b3a8-651262d62138"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력값 :  ['h', 'e', 'l', 'l', 'o']\n",
      "출력값 :  ['e', 'l', 'l', 'o', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "input_values = \"hello\"\n",
    "print(\"입력값 : \",list(input_values))\n",
    "input_vecs = np.stack([char2vec[char] \n",
    "                       for char in input_values])[np.newaxis]\n",
    "\n",
    "results = model.predict(input_vecs)\n",
    "result_indices = np.argmax(results,axis=-1)[0]\n",
    "print(\"출력값 : \",[idx2char[idx] for idx in result_indices])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2. RNN에서의 FeedForward.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
