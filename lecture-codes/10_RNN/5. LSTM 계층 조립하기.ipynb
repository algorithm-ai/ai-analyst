{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference List\n",
    "\n",
    "1. [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "2. 밑바닥부터 시작하는 딥러닝 2\n",
    "3. [Keras 내 LSTM 구현체](https://github.com/keras-team/keras/blob/master/keras/layers/recurrent.py#L1766)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "╔══<i><b>Alai-DeepLearning</b></i>════════════════════════════╗\n",
    "###  &nbsp;&nbsp; **✎&nbsp;&nbsp;Week 13. RNN-Basis**\n",
    "# Section 5. LSTM 계층 조립하기\n",
    "\n",
    "### _Objective_\n",
    "\n",
    "1. LSTM은 기존의 RNN과 달리, 상태(state)와 기억(Memory)을 나누어 관리하는 것이 특징입니다.<br>\n",
    "2. 이 모델은 RNNCell의 핵심 문제인 장기의존 관계를 효과적으로 해결했습니다. <br>\n",
    "3. LSTM은 현대 딥러닝 모델에 있어서, 기본적으로 쓰이는 RNN Cell로 자리매김하였습니다.\n",
    "\n",
    "\n",
    "╚═════════════════════════════════════════╝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ksj/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# \\[ 1. LSTM이란? \\]\n",
    "---\n",
    "---\n",
    "\n",
    "> *RNN의 핵심 이슈 중 하나인 BPTT(Back Propagation Through Time)는 Input과 output 사이의 긴 Time Step이 존재할 때, 학습을 어렵게 합니다.*<br>\n",
    "> *이러한 이슈를 통칭하여 장기기억 이슈라 하여, Time Step 간의 길이가 길 때 어떻게 효과적으로 학습시킬지가 관건입니다.*<br>\n",
    "> *RNN의 Cell을 개선하여 이를 효과적으로 해결한 것이 바로 LSTM입니다.*<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1. RNN 복습하기, RNNCell\n",
    "\n",
    "----\n",
    "\n",
    "* 아래는 이전 시간에 배웠던 RNNCell의 기본 형태입니다. RNN은 RNNCell을 time step 별로 반복하여 연산하는 구조를 가지고 있습니다.\n",
    "\n",
    "![Imgur](https://imgur.com/RNEHbhZ.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(Layer):\n",
    "    \"\"\"\n",
    "    Keras에서 Cell을 구성하는 Layer\n",
    "    \n",
    "    1. __init__ : Cell에 필요한 hyper-parameter를 구성\n",
    "       상태 벡터의 크기를 의미하는 self.state_size는 필수적으로 정해주어야 함\n",
    "       마지막에 super([Layer],self).__init__(**kwargs)를 호출해야함\n",
    "\n",
    "    2. build : Cell에서 관리하는 Weight들을 선언. \n",
    "       마지막에 super([Layer],self).build()를 호출해야함\n",
    "    \n",
    "    3. call : Cell의 연산 순서를 정의\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, n_units, **kwargs):\n",
    "        self.n_units = n_units\n",
    "        self.state_size = n_units\n",
    "        super(RNNCell, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.w_xh = self.add_weight(name='weight_xh',\n",
    "                                    shape=(input_shape[-1], self.n_units),\n",
    "                                    initializer='glorot_normal')\n",
    "        self.w_hh = self.add_weight(name='weight_hh',\n",
    "                                    shape=(self.n_units, self.n_units),\n",
    "                                    initializer='orthogonal')\n",
    "        self.b_h = self.add_weight(name='bias_h',\n",
    "                                   shape=(self.n_units,),\n",
    "                                   initializer='zeros')\n",
    "        super(RNNCell, self).build(input_shape)\n",
    "        \n",
    "    def call(self, inputs, states):\n",
    "        prev_states = states[0]\n",
    "        h = (tf.matmul(inputs, self.w_xh) \n",
    "             + tf.matmul(prev_states, self.w_hh)\n",
    "             + self.b_h)\n",
    "        a = tf.tanh(h)\n",
    "        return a, [a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력값과 출력값 사이의 간격(time steps)가 길다면, 어떤 문제가 발생할까요? 중요한 입력값은 state을 가지고 전파되는데 여기서 weight들이 계속 적용되면서 중요한 입력값은 점점 그 값을 소실하는 문제가 발생합니다. 이러한 문제는 time step의 길이가 10이나 20정도만 되어도 기존의 RNN은 경사하강법으로 훈련해서 학습에 성공할 확률이 매우 낮아지게 됩니다.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. LSTM의 인터페이스 \n",
    "\n",
    "---\n",
    "\n",
    "![Imgur](https://imgur.com/4m5fgb9.png)\n",
    "\n",
    "* LSTM Cell에는 기존 RNN과 달리 기억 셀(Memory Cell)이 존재하며, LSTM 전용의 기억 메커니즘을 가지고 있습니다.\n",
    "* LSTM 계층의 인터페이스에는 C라는 기억 셀이 존재합니다. 이것은 LSTM 계층 사이에서의 전용 `기억` 정보로, 자기 자신으로만 주고받는 것입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(Layer):\n",
    "    def __init__(self, n_units, **kwargs):\n",
    "        self.n_units = n_units\n",
    "        # 두 개의 State (C,H)를 가지기 때문에 \n",
    "        # self.state_size는 (n_units, n_units)가 됩니다.\n",
    "        self.state_size = (n_units, n_units)\n",
    "        super(LSTMCell, self).__init__(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. LSTM 내 Gate\n",
    "\n",
    "---\n",
    "\n",
    "![Imgur](https://imgur.com/iAKgtRs.png)\n",
    "\n",
    "* Sigmoid 함수로 이루어진 Gate는 0~1 범위의 출력값을 가지고 있습니다. Gate는 데이터의 흐름을 제어합니다. 데이터가 흘러가지 못하게 할 때에는 Gate의 값이 0으로 줄이고, 데이터가 흐르게 할때에는 Gate의 값을 1로 만듭니다.\n",
    "\n",
    "* LSTM에는 총 3개의 Gate가 존재합니다. 하나는 출력의 데이터를 제어하는 Output GATE, 그리고 기억을 지우는 Forget GATE, 마지막으로 기억을 더하는 Input GATE로 나뉘어집니다. 우리는 이것을 바탕으로 하나씩 구성해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) LSTM의 Output GATE\n",
    "\n",
    "![Imgur](https://imgur.com/k7uvjsb.png)\n",
    "\n",
    "* 기억 정보에는 시각 t에서의 LSTM의 기억이 저장되어 있습니다. 이러한 기억을 바탕으로, 외부 계층으로 빠져나가는 은닉 상태 $h_t$를 결정합니다. 이 때 출력하는 $h_t$는 아래와 같이 기억 셀의 값($c_t$)을 tanh 함수로 변환한 값입니다.\n",
    "* 단순히 기억의 값을 출력하는 것이 아니라, output GATE는 \"**기억의 요소 중 어떤 것들을 출력할까**\"를 결정하게 됩니다. 현재 상태와 이전 기억을 종합해서 값을 출력하게 됩니다.\n",
    "\n",
    "    $\n",
    "    {gate}^{(o)} = \\sigma(x_t\\cdot W_x^{(o)}+ h_t\\cdot W_h^{(o)}+b^{(o)}) \\\\\n",
    "    output = {gate}^{(o)} \\odot tanh(c_t)\n",
    "    $\n",
    "    \n",
    "\n",
    "* 수식에 존재하는 $\\odot$은 아마다르곱이라고 부르는 연산자로, 같은 크기의 두 행렬의 각 성분을 곱하는 연산입니다. Numpy 연산 중 `*`를 떠올리면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(Layer):\n",
    "    def __init__(self, n_units, **kwargs):\n",
    "        self.n_units = n_units\n",
    "        self.state_size = (n_units, n_units)\n",
    "        super(LSTMCell, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # output gate에 관련된 weight 선언\n",
    "        self.wx_output = self.add_weight(\"weight_x_output\",\n",
    "                                         shape=(input_shape[-1],self.n_units),\n",
    "                                         initializer='glorot_uniform')\n",
    "        self.wh_output = self.add_weight('weight_h_output',\n",
    "                                         shape=(self.n_units,self.n_units),\n",
    "                                         initializer='glorot_uniform')        \n",
    "        self.b_output = self.add_weight('bias_output',\n",
    "                                        shape=(self.n_units,),\n",
    "                                        initializer='zeros')\n",
    "        super(LSTMCell, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x, states):\n",
    "        h, c = states\n",
    "        \n",
    "        # output에 관련된 처리들\n",
    "        output_gate = tf.sigmoid(tf.dot(x,self.wx_output)+\n",
    "                                 tf.dot(h,self.wh_output)+\n",
    "                                 self.b_output)\n",
    "        new_h = output_gate * tf.tanh(c)\n",
    "        return new_h, [new_h, c]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) LSTM의 Forget GATE\n",
    "\n",
    "![Imgur](https://imgur.com/HxrGcW5.png)\n",
    "\n",
    "* 기억해야 할 것과 기억하지 말아야할 것을 분리해야 합니다. 기억 셀에 있는 것 중 현재 상태로 미루어보아, 불 필요한 부분들은 제거할 필요가 있습니다.\n",
    "* forget Gate는 기억 셀에서 무엇을 잊을까를 명확하게 지시하는 것입니다.\n",
    "\n",
    "    $\n",
    "    {gate}^{(f)} = \\sigma(x_t\\cdot W_x^{(f)}+ h_t\\cdot W_h^{(f)}+b^{(f)}) \\\\\n",
    "    c'_{t-1} = {gate}^{(f)} \\odot c_{t-1}\n",
    "    $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(Layer):\n",
    "    def __init__(self, n_units, **kwargs):\n",
    "        self.n_units = n_units\n",
    "        self.state_size = (n_units, n_units)\n",
    "        super(LSTMCell, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Forget Gate에 관련된 weight 선언\n",
    "        self.wx_forget = self.add_weight(\"weight_x_forget\",\n",
    "                                         shape=(input_shape[-1],self.n_units),\n",
    "                                         initializer='glorot_uniform')\n",
    "        self.wh_forget = self.add_weight('weight_h_forget',\n",
    "                                         shape=(self.n_units,self.n_units),\n",
    "                                         initializer='orthogonal')\n",
    "        self.b_forget = self.add_weight('bias_forget',\n",
    "                                        shape=(self.n_units,),\n",
    "                                        initializer='zeros')\n",
    "        \n",
    "        # output gate에 관련된 weight 선언\n",
    "        self.wx_output = self.add_weight(\"weight_x_output\",\n",
    "                                         shape=(input_shape[-1],self.n_units),\n",
    "                                         initializer='glorot_uniform')\n",
    "        self.wh_output = self.add_weight('weight_h_output',\n",
    "                                         shape=(self.n_units,self.n_units),\n",
    "                                         initializer='orthogonal')\n",
    "        self.b_output = self.add_weight('bias_output',\n",
    "                                        shape=(self.n_units,),\n",
    "                                        initializer='zeros')\n",
    "        super(LSTMCell, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x, states):\n",
    "        h, c = states\n",
    "        # forget에 관련된 처리들\n",
    "        forget_gate = tf.sigmoid(tf.dot(x,self.wx_forget)+\n",
    "                                 tf.dot(h,self.wh_forget)+\n",
    "                                 self.b_forget)\n",
    "        c = forget_gate * c        \n",
    "        \n",
    "        # output에 관련된 처리들\n",
    "        output_gate = tf.sigmoid(tf.dot(x,self.wx_output)+\n",
    "                                 tf.dot(h,self.wh_output)+\n",
    "                                 self.b_output)\n",
    "        new_h = output_gate * tf.tanh(c)\n",
    "        return new_h, [new_h, c]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) LSTM의 Input GATE\n",
    "\n",
    "![Imgur](https://imgur.com/g9iI4We.png)\n",
    "\n",
    "* 마지막으로 새로 기억해야 할 정보를 기억 셀에 추가하는 작업이 필요합니다.<br>\n",
    "* 그리고 기억할 요소들의 가치 판단은 input 게이트에서 합니다. 단순히 모든 기억할 요소들을 저장하는 것이 아닌, input Gate에서 필요한 것만 취사선택하도록 학습합니다.<br>\n",
    "* 즉 갱신할 정보를 만드는 `tanh` 부분과 갱신할 정보의 수준을 조절하는 `sigmoid` 부분으로 나누어 볼 수 있습니다.\n",
    "\n",
    "    $\n",
    "    update = tanh(x_t\\cdot W_x^{(u)}+ h_t\\cdot W_h^{(u)}+b^{(u)})\\\\\n",
    "    gate^{(i)} = \\sigma(x_t\\cdot W_x^{(i)}+ h_t\\cdot W_h^{(i)}+b^{(i)}) \\\\\n",
    "    c_t = c'_{t-1} + update \\odot {gate}^{(i)}\n",
    "    $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(Layer):\n",
    "    def __init__(self, n_units, **kwargs):\n",
    "        self.n_units = n_units\n",
    "        self.state_size = (n_units, n_units)\n",
    "        super(LSTMCell, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Forget Gate에 관련된 weight 선언\n",
    "        self.wx_forget = self.add_weight(\"weight_x_forget\",\n",
    "                                         shape=(input_shape[-1],self.n_units),\n",
    "                                         initializer='glorot_uniform')\n",
    "        self.wh_forget = self.add_weight('weight_h_forget',\n",
    "                                         shape=(self.n_units,self.n_units),\n",
    "                                         initializer='orthogonal')\n",
    "        self.b_forget = self.add_weight('bias_forget',\n",
    "                                        shape=(self.n_units,),\n",
    "                                        initializer='zeros')\n",
    "        # update에 관련된 weight 선언\n",
    "        self.wx_update = self.add_weight(\"weight_x_update\",\n",
    "                                         shape=(input_shape[-1],self.n_units),\n",
    "                                         initializer='glorot_uniform')\n",
    "        self.wh_update = self.add_weight('weight_h_update',\n",
    "                                         shape=(self.n_units,self.n_units),\n",
    "                                         initializer='orthogonal')\n",
    "        self.b_update = self.add_weight('bias_update',\n",
    "                                        shape=(self.n_units,),\n",
    "                                        initializer='zeros')\n",
    "        # Input Gate에 관련된 Weight 선언\n",
    "        self.wx_input = self.add_weight(\"weight_x_input\",\n",
    "                                         shape=(input_shape[-1],self.n_units),\n",
    "                                         initializer='glorot_uniform')\n",
    "        self.wh_input = self.add_weight('weight_h_input',\n",
    "                                         shape=(self.n_units,self.n_units),\n",
    "                                         initializer='orthogonal')\n",
    "        self.b_input = self.add_weight('bias_input',\n",
    "                                        shape=(self.n_units,),\n",
    "                                        initializer='zeros')\n",
    "        # output gate에 관련된 weight 선언\n",
    "        self.wx_output = self.add_weight(\"weight_x_output\",\n",
    "                                         shape=(input_shape[-1],self.n_units),\n",
    "                                         initializer='glorot_uniform')\n",
    "        self.wh_output = self.add_weight('weight_h_output',\n",
    "                                         shape=(self.n_units,self.n_units),\n",
    "                                         initializer='orthogonal')\n",
    "        self.b_output = self.add_weight('bias_output',\n",
    "                                        shape=(self.n_units,),\n",
    "                                        initializer='zeros')\n",
    "        super(LSTMCell, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x, states):\n",
    "        h, c = states\n",
    "        # forget에 관련된 처리들\n",
    "        forget_gate = tf.sigmoid(tf.dot(x,self.wx_forget)+\n",
    "                                 tf.dot(h,self.wh_forget)+\n",
    "                                 self.b_forget)\n",
    "        # update에 관련된 처리들\n",
    "        update = tf.tanh(tf.dot(x,self.wx_update)+\n",
    "                         tf.dot(h,self.wh_update)+\n",
    "                         self.b_update)\n",
    "        input_gate = tf.sigmoid(tf.dot(x,self.wx_input)+\n",
    "                                tf.dot(h,self.wh_input)+\n",
    "                                self.b_input)\n",
    "        new_c = forget_gate * c + update * input_gate\n",
    "        # output에 관련된 처리들\n",
    "        output_gate = tf.sigmoid(tf.dot(x,self.wx_output)+\n",
    "                                 tf.dot(h,self.wh_output)+\n",
    "                                 self.b_output)\n",
    "        new_h = output_gate * tf.tanh(new_c)\n",
    "        return new_h, [new_h, new_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "\n",
    "---\n",
    "\n",
    "    Copyright(c) 2019 by Public AI. All rights reserved.<br>\n",
    "    Writen by PAI, SangJae Kang ( rocketgrowthsj@publicai.co.kr )  last updated on 2019/06/18\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
